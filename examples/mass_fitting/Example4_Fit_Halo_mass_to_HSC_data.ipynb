{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e320d2bf-dce6-4a5b-ab38-04b0c948155f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example 4: Using a real dataset (HSC)\n",
    "## Fit halo mass to shear profile using HSC data\n",
    "\n",
    "_the LSST-DESC CLMM team_\n",
    "\n",
    "This notebook can be run on NERSC.\n",
    "\n",
    "Here we demonstrate how to run CLMM on real observational datasets. As an example, we use the data from the Hyper Suprime-Cam Subaru Strategic Program (HSC SSP) public releases (Aihara+2018ab, 2019; Mandelbaum+2018ab) (Credit: NAOJ / HSC Collaboration), which have similar observation conditions and data formats to the Rubin LSST.\n",
    "\n",
    "The steps in this notebook includes:\n",
    "- [Setting things up](#Setup)\n",
    "- [Selecting a cluster](#Selecting_a_cluster)\n",
    "- [Downloading the published catalog at the cluster field](#Downloading_the_catalog)\n",
    "- [Loading the catalog into CLMM](#Loading_the_catalog)\n",
    "- [Running CLMM on the dataset](#Running_CLMM)\n",
    "\n",
    "Links:\n",
    "\n",
    "The data access of the HSC SSP Public Data Release: \n",
    "https://hsc-release.mtk.nao.ac.jp/doc/index.php/data-access__pdr3/\n",
    "\n",
    "Shape catalog: \n",
    "https://hsc-release.mtk.nao.ac.jp/doc/index.php/s16a-shape-catalog-pdr2/\n",
    "\n",
    "FAQ: \n",
    "https://hsc-release.mtk.nao.ac.jp/doc/index.php/faq__pdr3/\n",
    "\n",
    "Photometric redshifts:\n",
    "https://hsc-release.mtk.nao.ac.jp/doc/index.php/photometric-redshifts/\n",
    "\n",
    "Cluster catalog:\n",
    "https://hsc-release.mtk.nao.ac.jp/doc/index.php/camira_pdr2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8f003-be65-44a3-8b0a-04749d1f7f13",
   "metadata": {},
   "source": [
    "<a id=\"Setup\"></a>\n",
    "## 1. Setup\n",
    "    \n",
    "We import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3503a1a-103f-48aa-b600-ef2d72de82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "from astropy.table import Table\n",
    "import pickle as pkl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e228cf4-f5f3-4b55-97b7-f1e022a5b29c",
   "metadata": {},
   "source": [
    "<a id=\"Selecting_a_cluster\"></a>\n",
    "## 2. Selecting a cluster\n",
    "\n",
    "We use the HSC SSP publications (https://hsc.mtk.nao.ac.jp/ssp/publications/) to select a list of reported massive galaxy clusters that have been measured by weak lensing. In the table below, the coordinates are for lensing peaks unless otherwise specified, and we assume h=0.7.\n",
    "\n",
    "Name | $$z_{cl}$$ | RA (deg) | DEC (deg) | WL Mass | Reference | Note\n",
    "- | - | - | - | - | - | -\n",
    "HWL16a-094 | 0.592 | 223.0801 | 0.1689 | 15.3, 7.8 | [Hamana+2020](https://ui.adsabs.harvard.edu/abs/2020PASJ...72...78H/abstract) | CAMIRA ID 1417; Miyazaki+2018 rank 34 \n",
    "HWL16a-026 | 0.424 | 130.5895 | 1.6473 | 8.7, 4.7  | [Hamana+2020](https://ui.adsabs.harvard.edu/abs/2020PASJ...72...78H/abstract) | --\n",
    "HWL16a-034 | 0.315 | 139.0387 | −0.3966 | 8.1, 5.6 | [Hamana+2020](https://ui.adsabs.harvard.edu/abs/2020PASJ...72...78H/abstract) | Abell 776; MACS J0916.1−0023; Miyazaki+2018 rank 8; see also Medezinski+2018 \n",
    "Rank 9 | 0.312 | 37.3951 | −3.6099 | --, 5.9 | [Miyazaki+2018](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..27M/abstract) | --\n",
    "Rank 48 | 0.529 | 220.7900 | 1.0509 | --, 10.4 | [Miyazaki+2018](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..27M/abstract) | --\n",
    "Rank 62 | 0.592 | 216.6510 | 0.7982 | --, 10.2 | [Miyazaki+2018](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..27M/abstract) | --\n",
    "MaxBCG J140.53188+03.76632 | 0.2701 | 140.54565 | 3.77820 | 44.3, 25.1 | [Medezinski+2018](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..28M%2F/abstract) | BCG center (close to the X-ray center); PSZ2 G228.50+34.95; double BCGs\n",
    "XLSSC006 | 0.429 | 35.439 | −3.772 | 9.6, 5.6 | [Umetsu+2020](https://ui.adsabs.harvard.edu/abs/2020ApJ...890..148U/abstract) | X-ray center\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684515e-e176-4b92-9509-0217ade681a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"Downloading_the_catalog\"></a>\n",
    "## 3. Downloading the catalog at the cluster field\n",
    "\n",
    "The 3 most massive cluster-candidates are MaxBCG.J140.53188+03.76632 (in the GAMA09H field), Miyazaki+2018 (M18 hearafter) rank 48 and 62 (in the GAMA15H field). We consider MaxBCG.J140.53188+03.76632 first.\n",
    "The webpage for HSC SSP data access is here [link](https://hsc-release.mtk.nao.ac.jp/doc/index.php/data-access__pdr3/).\n",
    "To download the catalogs, we need to first register for a user account ([link](https://hsc-release.mtk.nao.ac.jp/datasearch/new_user/new)).\n",
    "Then we log into the system, query and download the catalogs at [CAS Search](https://hsc-release.mtk.nao.ac.jp/datasearch/helps/sql_search); we use `object_id` to cross match the shape catalog, photo-z catalog, and photometry catalog. \n",
    "Since the clusters are at redshift about 0.4, a radius of 10 arcmin would be about 3 Mpc. However, we make a query for the whole field to save time.\n",
    "The final catalog includes shape info, photo-z, and photometry. \n",
    "Here is an example of the query SQL command (thank Calum Murray; [example command](https://hsc-release.mtk.nao.ac.jp/doc/index.php/s16a-shape-catalog-pdr2/); [schema](https://hsc-release.mtk.nao.ac.jp/schema/)); the query could take 1 hour and the size of the catalog could be 400 MB (.csv.gz). If you would like to test it, please copy from \"select\" to \"--LIMIT 5\". Also select \"PDR1\" or press \"Guess release from your SQL\" at the [CAS Search](https://hsc-release.mtk.nao.ac.jp/datasearch/helps/sql_search) webpage.\n",
    "To unpress the file \".gz\", use \"gunzip\" or \"gzip -d\".\n",
    "\n",
    "```\n",
    "select\n",
    " b.*, \n",
    " c.ira, c.idec, \n",
    " a.ishape_hsm_regauss_e1, a.ishape_hsm_regauss_e2, \n",
    " a.ishape_hsm_regauss_resolution, a.ishape_hsm_regauss_sigma, \n",
    " d1.photoz_best as ephor_ab_photoz_best, d1.photoz_risk_best as ephor_ab_photoz_risk_best, \n",
    " d2.photoz_best as frankenz_photoz_best, d2.photoz_risk_best as frankenz_photoz_risk_best, \n",
    " d3.photoz_best as nnpz_photoz_best, d3.photoz_risk_best as nnpz_photoz_risk_best, \n",
    " e.icmodel_mag, e.icmodel_mag_err, \n",
    " e.detect_is_primary, \n",
    " e.iclassification_extendedness, \n",
    " e.icmodel_flux_flags, \n",
    " e.icmodel_flux, e.icmodel_flux_err, \n",
    " c.iblendedness_abs_flux\n",
    "from\n",
    " s16a_wide.meas2 a\n",
    " inner join s16a_wide.weaklensing_hsm_regauss b using (object_id)\n",
    " inner join s16a_wide.meas c using (object_id)\n",
    " -- inner join s16a_wide.photoz_demp d using (object_id)\n",
    " -- inner join s16a_wide.photoz_ephor d using (object_id)\n",
    "  inner join s16a_wide.photoz_ephor_ab d1 using (object_id)\n",
    "  inner join s16a_wide.photoz_frankenz d2 using (object_id)\n",
    " -- inner join s16a_wide.photoz_mizuki d using (object_id)\n",
    " -- inner join s16a_wide.photoz_mlz d using (object_id)\n",
    "  inner join s16a_wide.photoz_nnpz d3 using (object_id)\n",
    "  inner join s16a_wide.forced e using (object_id)\n",
    "-- Uncomment the specific lines depending upon the field to be used\n",
    " -- where s16a_wide.search_xmm(c.skymap_id)\n",
    " -- where s16a_wide.search_wide01h(c.skymap_id)\n",
    " -- where s16a_wide.search_vvds(c.skymap_id)\n",
    " -- where s16a_wide.search_hectomap(c.skymap_id)\n",
    " -- where s16a_wide.search_gama15h(c.skymap_id)\n",
    " where s16a_wide.search_gama09h(c.skymap_id)\n",
    " --AND e.detect_is_primary\n",
    " --AND conesearch(c.icoord, 140.54565, 3.77820, 600) \n",
    " --AND NOT e.icmodel_flux_flags\n",
    " --AND e.iclassification_extendedness>0.5\n",
    " --LIMIT 5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4535f6-23b6-46e8-b9f8-7aeb899bfe7c",
   "metadata": {},
   "source": [
    "<a id=\"Loading_the_catalog\"></a>\n",
    "## 4. Loading the catalog into CLMM\n",
    "\n",
    "Once we have the catalog, we read in the catalog, make cuts on the catalog, and adjust column names to prepare for the analysis in CLMM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d966e-e00b-4e07-94d9-c3cc13b6183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Assume the downloaded catalog is at this path:\n",
    "filename = \"197376_GAMMA09H.csv\"\n",
    "catalog = filename.replace(\".csv\", \".pkl\")\n",
    "if not Path(catalog).is_file():\n",
    "    data_0 = Table.read(filename, format=\"ascii.csv\")\n",
    "    pkl.dump(data_0, open(catalog, \"wb\"))\n",
    "else:\n",
    "    data_0 = pkl.load(open(catalog, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e10ef1-f035-462b-98c6-ceac1d32a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_0.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58147358-bd97-4d81-9111-4909afb67e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select \"frankenz\" for the test, but there are other methods available.\n",
    "photoz_type = \"frankenz\"\n",
    "# photoz_type = \"nnpz\"\n",
    "# photoz_type = \"ephor_ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c53e2-aa5e-40e1-b46f-a955f0eb055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts\n",
    "def make_cuts(catalog_in):\n",
    "    # We consider some cuts in Mandelbaum et al. 2018 (HSC SSP Y1 shear catalog).\n",
    "    select = catalog_in[\"detect_is_primary\"] == \"True\"\n",
    "    select &= catalog_in[\"icmodel_flux_flags\"] == \"False\"\n",
    "    select &= catalog_in[\"iclassification_extendedness\"] > 0.5\n",
    "    select &= catalog_in[\"icmodel_mag_err\"] <= 2.5 / np.log(10.0) / 10.0\n",
    "    select &= (\n",
    "        catalog_in[\"ishape_hsm_regauss_e1\"] ** 2 + catalog_in[\"ishape_hsm_regauss_e2\"] ** 2 < 4.0\n",
    "    )\n",
    "    select &= catalog_in[\"icmodel_mag\"] <= 24.5\n",
    "    select &= catalog_in[\"iblendedness_abs_flux\"] < (10 ** (-0.375))\n",
    "    select &= catalog_in[\"ishape_hsm_regauss_resolution\"] >= 0.3  # similar to extendedness\n",
    "    select &= catalog_in[\"ishape_hsm_regauss_sigma\"] <= 0.4\n",
    "    # Note \"zbest\" minimizes the risk of the photo-z point estimate being far away from the true value.\n",
    "    # Details: https://hsc-release.mtk.nao.ac.jp/doc/wp-content/uploads/2017/02/pdr1_photoz_release_note.pdf\n",
    "    select &= catalog_in[\"%s_photoz_risk_best\" % photoz_type] < 0.5\n",
    "\n",
    "    catalog_out = catalog_in[select]\n",
    "\n",
    "    return catalog_out\n",
    "\n",
    "\n",
    "data_1 = make_cuts(data_0)\n",
    "print(len(data_0), len(data_1), len(data_1) * 1.0 / len(data_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2ae58-d538-479c-ac26-27dbf9838780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: Mandelbaum et al. 2018 \"The first-year shear catalog of the Subaru Hyper Suprime-Cam Subaru Strategic Program Survey\".\n",
    "# Section A.3.2: \"per-object galaxy shear estimate\".\n",
    "def apply_shear_calibration(catalog_in):\n",
    "    e1_0 = catalog_in[\"ishape_hsm_regauss_e1\"]\n",
    "    e2_0 = catalog_in[\"ishape_hsm_regauss_e2\"]\n",
    "    e_rms = catalog_in[\"ishape_hsm_regauss_derived_rms_e\"]\n",
    "    m = catalog_in[\"ishape_hsm_regauss_derived_shear_bias_m\"]\n",
    "    c1 = catalog_in[\"ishape_hsm_regauss_derived_shear_bias_c1\"]\n",
    "    c2 = catalog_in[\"ishape_hsm_regauss_derived_shear_bias_c2\"]\n",
    "    # Note: in the mass fit we have not implemented the weight yet.\n",
    "    weight = catalog_in[\"ishape_hsm_regauss_derived_shape_weight\"]\n",
    "\n",
    "    R = 1.0 - np.sum(weight * e_rms**2.0) / np.sum(weight)\n",
    "    m_mean = np.sum(weight * m) / np.sum(weight)\n",
    "    c1_mean = np.sum(weight * c1) / np.sum(weight)\n",
    "    c2_mean = np.sum(weight * c2) / np.sum(weight)\n",
    "    print(\"R, m_mean, c1_mean, c2_mean: \", R, m_mean, c1_mean, c2_mean)\n",
    "\n",
    "    g1 = (e1_0 / (2.0 * R) - c1) / (1.0 + m_mean)\n",
    "    g2 = (e2_0 / (2.0 * R) - c2) / (1.0 + m_mean)\n",
    "\n",
    "    return g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98993f-2261-452b-8515-a2c64d52c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust column names.\n",
    "def adjust_column_names(catalog_in):\n",
    "    # We consider a map between new and old column names.\n",
    "    # Note we have considered shear calibration here.\n",
    "    column_name_map = {\n",
    "        \"ra\": \"ira\",\n",
    "        \"dec\": \"idec\",\n",
    "        \"z\": \"%s_photoz_best\" % photoz_type,\n",
    "        \"id\": \"# object_id\",\n",
    "    }\n",
    "\n",
    "    catalog_out = Table()\n",
    "    for i in column_name_map:\n",
    "        catalog_out[i] = catalog_in[column_name_map[i]]\n",
    "\n",
    "    g1, g2 = apply_shear_calibration(catalog_in)\n",
    "    # CLMM uses \"epsilon shape\" rather than \"chi shape\".\n",
    "    catalog_out[\"e1\"] = g1\n",
    "    catalog_out[\"e2\"] = g2\n",
    "\n",
    "    return catalog_out\n",
    "\n",
    "\n",
    "data_2 = adjust_column_names(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d599e88-8a45-4b44-a46a-f2c00d86bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some figures for visualization.\n",
    "def make_plots(catalog_in):\n",
    "    # Scatter plot\n",
    "    plt.figure()\n",
    "    plt.scatter(catalog_in[\"ra\"], catalog_in[\"dec\"], c=catalog_in[\"z\"], s=1.0, alpha=0.2)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"ra\")\n",
    "    plt.ylabel(\"dec\")\n",
    "    plt.title(\"z\")\n",
    "\n",
    "    # Histogram\n",
    "    plt.figure()\n",
    "    plt.hist(catalog_in[\"z\"], bins=20)\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"count\")\n",
    "\n",
    "    # Relation\n",
    "    plt.figure()\n",
    "    plt.plot(catalog_in[\"e1\"], catalog_in[\"e2\"], \",\")\n",
    "    plt.xlabel(\"e1\")\n",
    "    plt.ylabel(\"e2\")\n",
    "\n",
    "\n",
    "# make_plots(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d6d00-f18e-4e27-969b-f02f13032713",
   "metadata": {},
   "source": [
    "<a id=\"Running_CLMM\"></a>\n",
    "## 5. Running CLMM on the dataset\n",
    "We use the functions similar to `examples/Paper_v1.0/gt_and_use_case.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b465d-9e1c-4b63-a43e-eeea6139b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm import Cosmology\n",
    "\n",
    "cosmo = Cosmology(H0=70.0, Omega_dm0=0.27 - 0.045, Omega_b0=0.045, Omega_k0=0.0)\n",
    "\n",
    "# We consider MaxBCG J140.53188+03.76632\n",
    "cluster_z = 0.2701  # Cluster redshift\n",
    "cluster_ra = 140.54565  # Cluster Ra in deg\n",
    "cluster_dec = 3.77820  # Cluster Dec in deg\n",
    "\n",
    "obs_galaxies = data_2\n",
    "\n",
    "obs_galaxies = obs_galaxies[(obs_galaxies[\"z\"] > (cluster_z + 0.1))]\n",
    "\n",
    "# Area cut: the query is made for the whole field and this can simplify the processing.\n",
    "select = obs_galaxies[\"ra\"] < cluster_ra + 12.0 / 60.0 / np.cos(cluster_dec / 180.0 * np.pi)\n",
    "select &= obs_galaxies[\"ra\"] > cluster_ra - 12.0 / 60.0 / np.cos(cluster_dec / 180.0 * np.pi)\n",
    "select &= obs_galaxies[\"dec\"] < cluster_dec + 12.0 / 60.0\n",
    "select &= obs_galaxies[\"dec\"] > cluster_dec - 12.0 / 60.0\n",
    "obs_galaxies = obs_galaxies[select]\n",
    "\n",
    "obs_galaxies[\"id\"] = np.arange(len(obs_galaxies))\n",
    "\n",
    "# Put galaxy values on arrays.\n",
    "gal_ra = obs_galaxies[\"ra\"]  # Galaxies Ra in deg\n",
    "gal_dec = obs_galaxies[\"dec\"]  # Galaxies Dec in deg\n",
    "gal_e1 = obs_galaxies[\"e1\"]  # Galaxies elipticipy 1\n",
    "gal_e2 = obs_galaxies[\"e2\"]  # Galaxies elipticipy 2\n",
    "gal_z = obs_galaxies[\"z\"]  # Galaxies observed redshift\n",
    "gal_id = obs_galaxies[\"id\"]  # Galaxies ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42573e50-85cc-4977-baf4-723711792c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the GalaxyCluster object.\n",
    "\n",
    "import clmm\n",
    "import clmm.dataops as da\n",
    "from clmm.utils import convert_units\n",
    "\n",
    "# Create a GCData with the galaxies.\n",
    "galaxies = clmm.GCData(\n",
    "    [gal_ra, gal_dec, gal_e1, gal_e2, gal_z, gal_id], names=[\"ra\", \"dec\", \"e1\", \"e2\", \"z\", \"id\"]\n",
    ")\n",
    "\n",
    "# Create a GalaxyCluster.\n",
    "cluster = clmm.GalaxyCluster(\"Name of cluster\", cluster_ra, cluster_dec, cluster_z, galaxies)\n",
    "\n",
    "# Convert elipticities into shears for the members.\n",
    "cluster.compute_tangential_and_cross_components()\n",
    "print(cluster.galcat.colnames)\n",
    "\n",
    "# Measure profile and add profile table to the cluster.\n",
    "seps = convert_units(cluster.galcat[\"theta\"], \"radians\", \"Mpc\", cluster.z, cosmo)\n",
    "\n",
    "cluster.make_radial_profile(\n",
    "    bins=da.make_bins(0.3, 3.0, 15, method=\"evenlog10width\"),\n",
    "    bin_units=\"Mpc\",\n",
    "    cosmo=cosmo,\n",
    "    include_empty_bins=False,\n",
    "    gal_ids_in_bins=True,\n",
    ")\n",
    "print(cluster.profile.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ebc79-67e8-43da-ad89-ca96666f541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_axes((0, 0, 1, 1))\n",
    "errorbar_kwargs = dict(linestyle=\"\", marker=\"o\", markersize=1, elinewidth=0.5, capthick=0.5)\n",
    "ax.errorbar(\n",
    "    cluster.profile[\"radius\"],\n",
    "    cluster.profile[\"gt\"],\n",
    "    cluster.profile[\"gt_err\"],\n",
    "    c=\"k\",\n",
    "    **errorbar_kwargs\n",
    ")\n",
    "ax.set_xlabel(\"r [Mpc]\", fontsize=10)\n",
    "ax.set_ylabel(r\"$g_t$\", fontsize=10)\n",
    "ax.grid(lw=0.3)\n",
    "ax.minorticks_on()\n",
    "ax.grid(which=\"minor\", lw=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e51a27-936a-48f5-abc7-120cff89b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical predictions\n",
    "\n",
    "# Model relying on the overall redshift distribution of the sources (WtG III Applegate et al. 2014).\n",
    "# Note the concentration of MaxBCG J140.53188+03.76632 was not reported.\n",
    "# The value from the stacked sample in the paper is ~7.\n",
    "# For the mass scale, a typical c-M relation (e.g. Child et al. 2018) would give c~3 though.\n",
    "# And we have not considered a c-M relation in the fitting.\n",
    "z_inf = 1000\n",
    "concentration = 4.0\n",
    "\n",
    "bs_mean = np.mean(clmm.utils.compute_beta_s(cluster.galcat[\"z\"], cluster_z, z_inf, cosmo))\n",
    "bs2_mean = np.mean(clmm.utils.compute_beta_s(cluster.galcat[\"z\"], cluster_z, z_inf, cosmo) ** 2)\n",
    "\n",
    "\n",
    "def predict_reduced_tangential_shear_redshift_distribution(profile, logm):\n",
    "    gt = clmm.compute_reduced_tangential_shear(\n",
    "        r_proj=profile[\"radius\"],  # Radial component of the profile\n",
    "        mdelta=10**logm,  # Mass of the cluster [M_sun]\n",
    "        cdelta=concentration,  # Concentration of the cluster\n",
    "        z_cluster=cluster_z,  # Redshift of the cluster\n",
    "        z_source=(bs_mean, bs2_mean),  # tuple of (bs_mean, bs2_mean)\n",
    "        z_src_info=\"beta\",\n",
    "        approx=\"order1\",\n",
    "        cosmo=cosmo,\n",
    "        delta_mdef=200,\n",
    "        massdef=\"critical\",\n",
    "        halo_profile_model=\"nfw\",\n",
    "    )\n",
    "    return gt\n",
    "\n",
    "\n",
    "# Model using individual redshift and radial information, to compute the averaged shear in each radial bin, based on the galaxies actually present in that bin.\n",
    "cluster.galcat[\"theta_mpc\"] = convert_units(\n",
    "    cluster.galcat[\"theta\"], \"radians\", \"mpc\", cluster.z, cosmo\n",
    ")\n",
    "\n",
    "\n",
    "def predict_reduced_tangential_shear_individual_redshift(profile, logm):\n",
    "    return np.array(\n",
    "        [\n",
    "            np.mean(\n",
    "                clmm.compute_reduced_tangential_shear(\n",
    "                    # Radial component of each source galaxy inside the radial bin\n",
    "                    r_proj=cluster.galcat[radial_bin[\"gal_id\"]][\"theta_mpc\"],\n",
    "                    mdelta=10**logm,  # Mass of the cluster [M_sun]\n",
    "                    cdelta=concentration,  # Concentration of the cluster\n",
    "                    z_cluster=cluster_z,  # Redshift of the cluster\n",
    "                    # Redshift value of each source galaxy inside the radial bin\n",
    "                    z_source=cluster.galcat[radial_bin[\"gal_id\"]][\"z\"],\n",
    "                    cosmo=cosmo,\n",
    "                    delta_mdef=200,\n",
    "                    massdef=\"critical\",\n",
    "                    halo_profile_model=\"nfw\",\n",
    "                )\n",
    "            )\n",
    "            for radial_bin in profile\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83799a1-5122-4e89-b67d-8f691ff9d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass fitting\n",
    "\n",
    "mask_for_fit = cluster.profile[\"n_src\"] > 2\n",
    "data_for_fit = cluster.profile[mask_for_fit]\n",
    "\n",
    "from clmm.support.sampler import fitters\n",
    "\n",
    "\n",
    "def fit_mass(predict_function):\n",
    "    popt, pcov = fitters[\"curve_fit\"](\n",
    "        predict_function,\n",
    "        data_for_fit,\n",
    "        data_for_fit[\"gt\"],\n",
    "        data_for_fit[\"gt_err\"],\n",
    "        bounds=[10.0, 17.0],\n",
    "    )\n",
    "    logm, logm_err = popt[0], np.sqrt(pcov[0][0])\n",
    "    return {\n",
    "        \"logm\": logm,\n",
    "        \"logm_err\": logm_err,\n",
    "        \"m\": 10**logm,\n",
    "        \"m_err\": (10**logm) * logm_err * np.log(10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1175703-d2ea-4f13-9f01-3cee81d90f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the paper, the measured mass is 44.3 {+ 30.3} {- 19.9} * 10^14 Msun (M200c,WL).\n",
    "# For convenience, we consider a mean value for the errorbar.\n",
    "# We build a dictionary based on that result.\n",
    "m_paper = 44.3e14\n",
    "m_err_paper = 25.1e14\n",
    "logm_paper = np.log10(m_paper)\n",
    "logm_err_paper = m_err_paper / (10**logm_paper) / np.log(10)\n",
    "paper_value = {\n",
    "    \"logm\": logm_paper,\n",
    "    \"logm_err\": logm_err_paper,\n",
    "    \"m\": 10**logm_paper,\n",
    "    \"m_err\": (10**logm_paper) * logm_err_paper * np.log(10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59846f-a380-449d-a30b-5261085a34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_redshift_distribution = fit_mass(predict_reduced_tangential_shear_redshift_distribution)\n",
    "fit_individual_redshift = fit_mass(predict_reduced_tangential_shear_individual_redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b8ac6-7294-4e85-97ac-28b62512be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Best fit mass for N(z) model                     = {fit_redshift_distribution[\"m\"]:.3e} +/- {fit_redshift_distribution[\"m_err\"]:.3e} Msun'\n",
    ")\n",
    "print(\n",
    "    f'Best fit mass for individual redshift and radius = {fit_individual_redshift[\"m\"]:.3e} +/- {fit_individual_redshift[\"m_err\"]:.3e} Msun'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e71bf0-5aa8-4c48-8042-bd461430291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the results.\n",
    "def get_predicted_shear(predict_function, fit_values):\n",
    "    gt_est = predict_function(data_for_fit, fit_values[\"logm\"])\n",
    "    gt_est_err = [\n",
    "        predict_function(data_for_fit, fit_values[\"logm\"] + i * fit_values[\"logm_err\"])\n",
    "        for i in (-3, 3)\n",
    "    ]\n",
    "    return gt_est, gt_est_err\n",
    "\n",
    "\n",
    "gt_redshift_distribution, gt_err_redshift_distribution = get_predicted_shear(\n",
    "    predict_reduced_tangential_shear_redshift_distribution, fit_redshift_distribution\n",
    ")\n",
    "gt_individual_redshift, gt_err_individual_redshift = get_predicted_shear(\n",
    "    predict_reduced_tangential_shear_individual_redshift, fit_individual_redshift\n",
    ")\n",
    "\n",
    "gt_paper1, gt_err_paper1 = get_predicted_shear(\n",
    "    predict_reduced_tangential_shear_redshift_distribution, paper_value\n",
    ")\n",
    "gt_paper2, gt_err_paper2 = get_predicted_shear(\n",
    "    predict_reduced_tangential_shear_individual_redshift, paper_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e95d3-bc69-4cf7-8ac0-1788b74fe5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_redshift_distribution_dof = np.sum(\n",
    "    (gt_redshift_distribution - data_for_fit[\"gt\"]) ** 2 / (data_for_fit[\"gt_err\"]) ** 2\n",
    ") / (len(data_for_fit) - 1)\n",
    "chi2_individual_redshift_dof = np.sum(\n",
    "    (gt_individual_redshift - data_for_fit[\"gt\"]) ** 2 / (data_for_fit[\"gt_err\"]) ** 2\n",
    ") / (len(data_for_fit) - 1)\n",
    "\n",
    "print(f\"Reduced chi2 (N(z) model) = {chi2_redshift_distribution_dof}\")\n",
    "print(f\"Reduced chi2 (individual (R,z) model) = {chi2_individual_redshift_dof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acffc41-3f07-4a33-8f31-8b27b4363d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "gt_ax = fig.add_axes((0.25, 0.42, 0.7, 0.55))\n",
    "gt_ax.errorbar(\n",
    "    data_for_fit[\"radius\"], data_for_fit[\"gt\"], data_for_fit[\"gt_err\"], c=\"k\", **errorbar_kwargs\n",
    ")\n",
    "\n",
    "# Points in grey have not been used for the fit.\n",
    "gt_ax.errorbar(\n",
    "    cluster.profile[\"radius\"][~mask_for_fit],\n",
    "    cluster.profile[\"gt\"][~mask_for_fit],\n",
    "    cluster.profile[\"gt_err\"][~mask_for_fit],\n",
    "    c=\"grey\",\n",
    "    **errorbar_kwargs,\n",
    ")\n",
    "\n",
    "pow10 = 15\n",
    "mlabel = lambda name, fits: (\n",
    "    rf\"$M_{{fit}}^{{{name}}} = \"\n",
    "    rf'{fits[\"m\"]/10**pow10:.2f}\\pm'\n",
    "    rf'{fits[\"m_err\"]/10**pow10:.2f}'\n",
    "    rf\"\\times 10^{{{pow10}}} M_\\odot$\"\n",
    ")\n",
    "\n",
    "# The model for the 1st method.\n",
    "gt_ax.loglog(\n",
    "    data_for_fit[\"radius\"],\n",
    "    gt_redshift_distribution,\n",
    "    \"-C1\",\n",
    "    label=mlabel(\"N(z)\", fit_redshift_distribution),\n",
    "    lw=0.5,\n",
    ")\n",
    "gt_ax.fill_between(\n",
    "    data_for_fit[\"radius\"], *gt_err_redshift_distribution, lw=0, color=\"C1\", alpha=0.2\n",
    ")\n",
    "\n",
    "# The model for the 2nd method.\n",
    "gt_ax.loglog(\n",
    "    data_for_fit[\"radius\"],\n",
    "    gt_individual_redshift,\n",
    "    \"-C2\",\n",
    "    label=mlabel(\"z,R\", fit_individual_redshift),\n",
    "    lw=0.5,\n",
    ")\n",
    "gt_ax.fill_between(data_for_fit[\"radius\"], *gt_err_individual_redshift, lw=0, color=\"C2\", alpha=0.2)\n",
    "\n",
    "# The value in the reference paper.\n",
    "gt_ax.loglog(\n",
    "    data_for_fit[\"radius\"], gt_paper1, \"-C3\", label=mlabel(\"paper; N(z)\", paper_value), lw=0.5\n",
    ")\n",
    "gt_ax.fill_between(data_for_fit[\"radius\"], *gt_err_paper1, lw=0, color=\"C3\", alpha=0.2)\n",
    "\n",
    "gt_ax.loglog(\n",
    "    data_for_fit[\"radius\"], gt_paper2, \"-C4\", label=mlabel(\"paper; Z,R\", paper_value), lw=0.5\n",
    ")\n",
    "gt_ax.fill_between(data_for_fit[\"radius\"], *gt_err_paper2, lw=0, color=\"C4\", alpha=0.2)\n",
    "\n",
    "gt_ax.set_ylabel(r\"$g_t$\", fontsize=8)\n",
    "gt_ax.legend(fontsize=6)\n",
    "gt_ax.set_xticklabels([])\n",
    "gt_ax.tick_params(\"x\", labelsize=8)\n",
    "gt_ax.tick_params(\"y\", labelsize=8)\n",
    "\n",
    "\n",
    "errorbar_kwargs2 = {k: v for k, v in errorbar_kwargs.items() if \"marker\" not in k}\n",
    "errorbar_kwargs2[\"markersize\"] = 3\n",
    "errorbar_kwargs2[\"markeredgewidth\"] = 0.5\n",
    "res_ax = fig.add_axes((0.25, 0.2, 0.7, 0.2))\n",
    "delta = (cluster.profile[\"radius\"][1] / cluster.profile[\"radius\"][0]) ** 0.25\n",
    "\n",
    "\n",
    "res_ax.errorbar(\n",
    "    data_for_fit[\"radius\"],\n",
    "    data_for_fit[\"gt\"] / gt_redshift_distribution - 1,\n",
    "    yerr=data_for_fit[\"gt_err\"] / gt_redshift_distribution,\n",
    "    marker=\"s\",\n",
    "    c=\"C1\",\n",
    "    **errorbar_kwargs2,\n",
    ")\n",
    "errorbar_kwargs2[\"markersize\"] = 3\n",
    "errorbar_kwargs2[\"markeredgewidth\"] = 0.5\n",
    "\n",
    "res_ax.errorbar(\n",
    "    data_for_fit[\"radius\"] * delta,\n",
    "    data_for_fit[\"gt\"] / gt_individual_redshift - 1,\n",
    "    yerr=data_for_fit[\"gt_err\"] / gt_individual_redshift,\n",
    "    marker=\"*\",\n",
    "    c=\"C2\",\n",
    "    **errorbar_kwargs2,\n",
    ")\n",
    "res_ax.set_xlabel(r\"$R$ [Mpc]\", fontsize=8)\n",
    "\n",
    "res_ax.set_ylabel(r\"$g_t^{data}/g_t^{mod.}-1$\", fontsize=8)\n",
    "res_ax.set_xscale(\"log\")\n",
    "\n",
    "res_ax.set_ylim(-5, 5)\n",
    "res_ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "res_ax.tick_params(\"x\", labelsize=8)\n",
    "res_ax.tick_params(\"y\", labelsize=8)\n",
    "\n",
    "for ax in (gt_ax, res_ax):\n",
    "    ax.grid(lw=0.3)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which=\"minor\", lw=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8f5fb-82f0-490e-8fea-3a1f18b34311",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "Aihara, H., Arimoto, N., Armstrong, R., et al. 2018, Publications of the Astronomical Society of Japan, 70, S4\n",
    "\n",
    "Aihara, H., Armstrong, R., Bickerton, S., et al. 2018, Publications of the Astronomical Society of Japan, 70, S8\n",
    "\n",
    "Aihara, H., AlSayyad, Y., Ando, M., et al. 2019, Publications of the Astronomical Society of Japan, 71\n",
    "\n",
    "Hamana, T., Shirasaki, M., Lin, Y.-T., 2020, Publications of the Astronomical Society of Japan, 72, 78\n",
    "\n",
    "Mandelbaum, R., Miyatake, H., Hamana, T., et al. 2018, Publications of the Astronomical Society of Japan, 70, S25\n",
    "\n",
    "Mandelbaum, R., Lanusse, F., Leauthaud, A., et al. 2018, Monthly Notices of the Royal Astronomical Society, 481, 3170\n",
    "\n",
    "Medezinski, E., Battaglia, N., Umetsu, K., et al., 2018, Publications of the Astronomical Society of Japan, 70, S28\n",
    "\n",
    "Miyazaki, S., Oguri, M., Hamana, T., et al., 2018, Publications of the Astronomical Society of Japan, 70, S27\n",
    "\n",
    "Umetsu, K., Sereno, M., Lieu, M., et al., 2020, Astrophysical Journal, 890, 148\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
