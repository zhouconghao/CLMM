{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337ca38d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate \"realistic\" mock data for a cluster ensemble\n",
    "## Generate cluster ensemble with \"realistic\" mass and redshift distribution\n",
    "\n",
    "In this notebook, we generate a 'realistic' mock cluster ensemble, i.e. given a halo mass function, and mass-concentration relation (more complexity, such as miscentering, could also be added but it's not done here). The notebook is structured as follows:\n",
    "- Implementation of the CCL and NumCosmo mass functions.\n",
    "- Some comparisons between the different backends.\n",
    "- Implementation of a accept-reject method to generate cluster samples from the mass function.\n",
    "- Generation of a cluster catalog for mass and redshift bins followed by a comparison with the theoretical prediction\n",
    "\n",
    "Then, the remaining of the notebook repeats what was done in the `Example_cluster_ensemble.ipynb`.\n",
    "- Geration of mock galaxy data for each cluster\n",
    "- Creating ClusterEnsemble object and estimation of individual excess surface density profiles\n",
    "- An analysis of the covariance of the stack between radial bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a17a2-5038-4e2a-9f53-aaff51d7f96b",
   "metadata": {},
   "source": [
    "## Draw pairs of mass and redshift according to a mass function (computed with either CCL or NumCosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18c6e2",
   "metadata": {},
   "source": [
    "<!-- The halo mass function is given by\n",
    "$$\n",
    "\\frac{dn}{dM} = f(\\sigma) \\frac{\\bar{\\rho}_m}{M}\\frac{d ln\\sigma^{-1}}{dM}\n",
    "$$\n",
    "where $\\sigma$ is the variance, $\\bar{\\rho}_m$ is the mean density of the universe, $M$ is the halo mass and $f$ is the multiplicity function. In this notebook we use the Tinker multiplicity function \n",
    "$$\n",
    "f(\\sigma) = A \\left[ \\left(\\frac{\\sigma}{b}\\right)^{-a} +1\\right] e^{-\\frac{c}{\\sigma^2}}\n",
    ".$$\n",
    "In the above, $A$ is the normalization factor and $a,b$ and $c$ are parameters to be set by the model.  -->\n",
    "\n",
    "In this notebook we use the Tinker halo mass function, and demonstrate the implementation via CCL or NumCosmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565e71a-63e2-4047-b7e9-704bd3301e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pyccl as ccl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import clmm\n",
    "from clmm import GalaxyCluster, ClusterEnsemble, GCData\n",
    "from clmm import Cosmology\n",
    "from clmm.support import mock_data as mock\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aed85b-f1c2-4efb-be97-981ab022fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumCosmo imports\n",
    "try:\n",
    "    import gi\n",
    "\n",
    "    gi.require_version(\"NumCosmo\", \"1.0\")\n",
    "    gi.require_version(\"NumCosmoMath\", \"1.0\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from gi.repository import GObject\n",
    "from gi.repository import NumCosmo as Nc\n",
    "from gi.repository import NumCosmoMath as Ncm\n",
    "\n",
    "Ncm.cfg_init()\n",
    "Ncm.cfg_set_log_handler(lambda msg: sys.stdout.write(msg) and sys.stdout.flush())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5de193-4dc7-4eb6-a345-d6ffb6dfc952",
   "metadata": {},
   "source": [
    "For reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b949bce-ec10-4bd0-80f1-b00fd19c00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4907974-ad57-45cb-9482-d3e526e5e6a9",
   "metadata": {},
   "source": [
    "### Mass function - CCL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228aac90-b3a4-4510-a1f9-806314dbca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = ccl.Cosmology(\n",
    "    Omega_c=0.26,\n",
    "    Omega_b=0.04,\n",
    "    h=0.7,\n",
    "    sigma8=0.8,\n",
    "    n_s=0.96,\n",
    "    Neff=3.04,\n",
    "    m_nu=1.0e-05,\n",
    "    mass_split=\"single\",\n",
    ")\n",
    "hmd_200c = ccl.halos.MassDef(200, \"critical\")\n",
    "\n",
    "\n",
    "# For a different multiplicty function, the user must change this function below\n",
    "def tinker08_ccl(logm, z):\n",
    "    mass = 10 ** (logm)\n",
    "    hmf_200c = ccl.halos.MassFuncTinker08(mass_def=hmd_200c)\n",
    "    nm = hmf_200c(cosmo, mass, 1.0 / (1 + z))\n",
    "    return nm  # dn/dlog10M\n",
    "\n",
    "\n",
    "# Computing the volume element\n",
    "def dV_over_dOmega_dz(z):\n",
    "    a = 1.0 / (1.0 + z)\n",
    "    da = ccl.background.angular_diameter_distance(cosmo, a)\n",
    "    E = ccl.background.h_over_h0(cosmo, a)\n",
    "    return ((1.0 + z) ** 2) * (da**2) * ccl.physical_constants.CLIGHT_HMPC / cosmo[\"h\"] / E\n",
    "\n",
    "\n",
    "def pdf_tinker08_ccl(logm, z):\n",
    "    return tinker08_ccl(logm, z) * dV_over_dOmega_dz(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7f1d1-d33a-4229-bb0c-c19964380045",
   "metadata": {},
   "source": [
    "### Mass function - Numcosmo implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ec630-5bc6-44d6-b2fb-7b7e41d39712",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncm.cfg_init()\n",
    "# cosmo_nc = Nc.HICosmoDEXcdm()\n",
    "cosmo_nc = Nc.HICosmo.new_from_name(Nc.HICosmo, \"NcHICosmoDECpl{'massnu-length':<1>}\")\n",
    "cosmo_nc.omega_x2omega_k()\n",
    "cosmo_nc.param_set_by_name(\"w0\", -1.0)\n",
    "cosmo_nc.param_set_by_name(\"w1\", 0.0)\n",
    "cosmo_nc.param_set_by_name(\"Tgamma0\", 2.725)\n",
    "cosmo_nc.param_set_by_name(\"massnu_0\", 0.0)\n",
    "cosmo_nc.param_set_by_name(\"H0\", 70.0)\n",
    "cosmo_nc.param_set_by_name(\"Omegab\", 0.04)\n",
    "cosmo_nc.param_set_by_name(\"Omegac\", 0.26)\n",
    "cosmo_nc.param_set_by_name(\"Omegak\", 0.0)\n",
    "\n",
    "\n",
    "# ENnu = 3.046 - 3.0 * \\\n",
    "#     cosmo_nc.E2Press_mnu(1.0e10) / (cosmo_nc.E2Omega_g(1.0e10)\n",
    "#                                  * (7.0/8.0*(4.0/11.0)**(4.0/3.0)))\n",
    "\n",
    "ENnu = 3.046\n",
    "cosmo_nc.param_set_by_name(\"ENnu\", ENnu)\n",
    "reion = Nc.HIReionCamb.new()\n",
    "prim = Nc.HIPrimPowerLaw.new()\n",
    "\n",
    "cosmo_nc.add_submodel(reion)\n",
    "cosmo_nc.add_submodel(prim)\n",
    "\n",
    "dist = Nc.Distance.new(2.0)\n",
    "dist.prepare_if_needed(cosmo_nc)\n",
    "tf = Nc.TransferFunc.new_from_name(\"NcTransferFuncEH\")\n",
    "\n",
    "psml = Nc.PowspecMLTransfer.new(tf)\n",
    "psml.require_kmin(1.0e-6)\n",
    "psml.require_kmax(1.0e3)\n",
    "\n",
    "psf = Ncm.PowspecFilter.new(psml, Ncm.PowspecFilterType.TOPHAT)\n",
    "psf.set_best_lnr0()\n",
    "\n",
    "prim.props.n_SA = 0.96\n",
    "\n",
    "old_amplitude = np.exp(prim.props.ln10e10ASA)\n",
    "prim.props.ln10e10ASA = np.log((0.8 / cosmo_nc.sigma8(psf)) ** 2 * old_amplitude)\n",
    "print(0.8, cosmo_nc.sigma8(psf))\n",
    "\n",
    "mulf = Nc.MultiplicityFuncTinker.new()\n",
    "mulf.set_mdef(Nc.MultiplicityFuncMassDef.CRITICAL)\n",
    "mulf.set_Delta(200.0)\n",
    "mf = Nc.HaloMassFunction.new(dist, psf, mulf)\n",
    "\n",
    "\n",
    "#\n",
    "# New mass function object using the objects defined above.\n",
    "#\n",
    "def tinker08_nc(logm, z):\n",
    "    lnm = logm * np.log(10.0)  # convert log10(M) to ln(M)\n",
    "    res = mf.dn_dlnM(cosmo_nc, lnm, z)\n",
    "    return res * np.log(10.0)  # convert dn/dlnM to dn/dlog10M\n",
    "\n",
    "\n",
    "def pdf_tinker08_nc(logm, z):\n",
    "    return tinker08_nc(logm, z) * mf.dv_dzdomega(cosmo_nc, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513b91e-2777-49eb-82f6-d40f6546f98f",
   "metadata": {},
   "source": [
    "### Sanity Checks - compare the CCL and NC implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c866d-c030-4986-b904-3df5d886653c",
   "metadata": {},
   "source": [
    "#### Volume element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c88ad-4a0f-4bba-ba24-c4af4f645a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_arr = np.linspace(0.01, 2, 100)\n",
    "v_nc = [mf.dv_dzdomega(cosmo_nc, z) for z in z_arr]\n",
    "v_ccl = dV_over_dOmega_dz(z_arr)\n",
    "plt.plot(z_arr, np.abs(v_nc / v_ccl - 1) * 100)\n",
    "plt.xlabel(\"Redshift\", size=14)\n",
    "plt.ylabel(\"Relative difference [%]\", size=14)\n",
    "plt.grid(which=\"major\", lw=0.5)\n",
    "plt.grid(which=\"minor\", lw=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fefe7-c848-4676-8a9e-82eb53a03d45",
   "metadata": {},
   "source": [
    "#### Mass function\n",
    "\n",
    "The CCL and Numcosmo implementations give different results for the Tinker08 HMF, especially at high redshift (>1.5). Need to understand why, although it is not a regime we should find ourselves in with cluster studies at optical wavelength. Might be that the CCL and NC cosmologies defined above are not actually the same (neutrinos?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc75b1a-64c3-4df6-b8bb-1c77f7247b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logm_arr = np.linspace(14.0, 15)\n",
    "z_arr = np.linspace(0.01, 1.7, 2)\n",
    "\n",
    "t08_vec_ccl = np.vectorize(tinker08_ccl)\n",
    "t08_vec_nc = np.vectorize(tinker08_nc)\n",
    "for z in z_arr:\n",
    "    res_ccl = t08_vec_ccl(logm_arr, z)\n",
    "    res_nc = t08_vec_nc(logm_arr, z)\n",
    "    plt.plot(logm_arr, res_ccl, label=f\"z={z}, CCL\")\n",
    "    plt.plot(logm_arr, res_nc, label=f\"z={z}, NC\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"log10(M200,c)\", size=14)\n",
    "    plt.ylabel(\"dn/d$\\log_{10}$M\", size=14)\n",
    "plt.legend()\n",
    "plt.grid(which=\"major\", lw=0.5)\n",
    "plt.grid(which=\"minor\", lw=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f6a29-17c6-4d00-97d2-b75d3df17ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_arr = np.linspace(0.01, 1.7, 100)\n",
    "logm = 15.0\n",
    "res_ccl = t08_vec_ccl(logm, z_arr)\n",
    "res_nc = t08_vec_nc(logm, z_arr)\n",
    "plt.plot(z_arr, np.abs(res_ccl / res_nc - 1) * 100)\n",
    "plt.xlabel(\"Redshift\", size=14)\n",
    "plt.ylabel(\"Relative difference [%]\", size=14)\n",
    "plt.grid(which=\"major\", lw=0.5)\n",
    "plt.grid(which=\"minor\", lw=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed29aa-f030-4e4f-8b0e-71e6aa36b6b7",
   "metadata": {},
   "source": [
    "### Acceptance-rejection method to sample (M,z) from the mass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39352c6b-4a61-40c9-92ce-436f16fc8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_draw(\n",
    "    pdf, N=1000, logm_min=14.0, logm_max=15.0, zmin=0.01, zmax=1, Ngrid_m=30, Ngrid_z=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses the rejection method for generating random numbers derived from an arbitrary\n",
    "    probability distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf : func\n",
    "      2d distribution function to sample\n",
    "    N : int\n",
    "      number of points to generate\n",
    "    log_min,logm_max : float\n",
    "      log10 mass range\n",
    "    zmin,zmax : float\n",
    "      redshift range\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ran_logm : list\n",
    "        accepted logm values\n",
    "    ran_z : list\n",
    "        accepted redshift values\n",
    "    acceptance : float\n",
    "        acceptance ratio of the method\n",
    "    \"\"\"\n",
    "\n",
    "    # maximum value of the pdf over the mass and redshift space.\n",
    "    # the pdf is not monotonous in mass and redshift, so we need\n",
    "    # to find the maximum numerically. Here we scan the space\n",
    "    # with a regular grid and use the maximum value.\n",
    "    # Accuracy of the results depends on Ngrid_m and Ngrid_z\n",
    "    # This should probably be improved\n",
    "\n",
    "    logM_arr = np.linspace(logm_min, logm_max, Ngrid_m)\n",
    "    z_arr = np.logspace(np.log10(zmin), np.log10(zmax), Ngrid_z)\n",
    "    p = []\n",
    "    for logM in logM_arr:\n",
    "        for z in z_arr:\n",
    "            p.append(pdf(logM, z))\n",
    "    pmax = np.max(p)\n",
    "    pmin = np.min(p)\n",
    "    # Counters\n",
    "    naccept = 0\n",
    "    ntrial = 0\n",
    "\n",
    "    # Keeps drawing until N points are accepted\n",
    "    ran_logm = []  # output list of random numbers\n",
    "    ran_z = []  # output list of random numbers\n",
    "    while naccept < N:\n",
    "        # draw (logm,z) from uniform distribution\n",
    "        # draw p from uniform distribution\n",
    "        logm = np.random.uniform(logm_min, logm_max)  # x'\n",
    "        z = np.random.uniform(zmin, zmax)  # x'\n",
    "        p = np.random.uniform(0.0, pmax)  # y'\n",
    "\n",
    "        if p < pdf(logm, z):\n",
    "            # keep the point\n",
    "            #            print(logm, z, p, pdf(logm, z))\n",
    "            ran_logm.append(logm)\n",
    "            ran_z.append(z)\n",
    "            naccept = naccept + 1\n",
    "        ntrial = ntrial + 1\n",
    "\n",
    "    ran_logm = np.asarray(ran_logm)\n",
    "    ran_z = np.asarray(ran_z)\n",
    "\n",
    "    acceptance = float(N / ntrial)\n",
    "    print(f\"acceptance = {acceptance}\")\n",
    "    return ran_logm, ran_z, acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb95e78-5a04-4f37-ae86-b0e43bf50791",
   "metadata": {},
   "source": [
    "### Draw $10^4$ (M,z) pairs and check consistency with mass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77fae0-c49e-4b2d-9b27-297d5a8f72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numbers of pairs to draw and the mass and redshift ranges\n",
    "N = 10000\n",
    "logm_min = 14.0\n",
    "logm_max = 14.5\n",
    "zmin = 0.2\n",
    "zmax = 1.0\n",
    "\n",
    "# Choose between the CCL and NC implementation (NC is faster)\n",
    "# pdf = pdf_tinker08_ccl\n",
    "pdf = pdf_tinker08_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e64b9-32c8-4113-a132-8ed060ff7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation factor for the pdf to be used later\n",
    "norm = (\n",
    "    1.0\n",
    "    / scipy.integrate.dblquad(\n",
    "        pdf, zmin, zmax, lambda x: logm_min, lambda x: logm_max, epsrel=1.0e-4\n",
    "    )[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab886a3-568d-4f2b-a79a-5dfa5c86c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random draw\n",
    "ran_logm, ran_z, acceptance = bivariate_draw(\n",
    "    pdf, N=N, logm_min=logm_min, logm_max=logm_max, zmin=zmin, zmax=zmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519333e-3791-4b0a-b354-2956362d821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(ran_logm, ran_z, bins=10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1252ee-720f-4e8b-8e16-974690c2f0f7",
   "metadata": {},
   "source": [
    "### Check distribution in mass bins (integrate over the full redshift range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc8b4a-52a6-4705-9eb2-2b558aa53566",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_m = np.histogram(ran_logm, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a320b1-d000-49d9-bfa3-51d5835c1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zmin, zmax)\n",
    "predicted_T08_m_quad = [\n",
    "    norm\n",
    "    * N\n",
    "    * scipy.integrate.dblquad(pdf, zmin, zmax, lambda x: lmmin, lambda x: lmmax, epsrel=1.0e-4)[0]\n",
    "    for lmmin, lmmax in zip(hist_m[1], hist_m[1][1:])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41d9ba-0ca8-46a1-a8a5-13ddf396accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers_m = 0.5 * (hist_m[1][:-1] + hist_m[1][1:])\n",
    "\n",
    "plt.hist(\n",
    "    ran_logm,\n",
    "    bins=hist_m[1],\n",
    "    color=\"teal\",\n",
    "    alpha=0.3,\n",
    "    label=\"drawn masses - integrated over redshift range\",\n",
    ")\n",
    "plt.plot(\n",
    "    bin_centers_m, np.array(predicted_T08_m_quad), \"-o\", color=\"green\", label=\"Expected from theory\"\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"$\\log_{10}$M\", size=14)\n",
    "plt.ylabel(\"Number\", size=14)\n",
    "plt.title(f\"{N:,} clusters drawn in log10M={logm_min, logm_max}, z={zmin, zmax}\")\n",
    "plt.legend()\n",
    "plt.grid(which=\"major\", lw=0.5)\n",
    "plt.grid(which=\"minor\", lw=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d551ebc-ef0c-46bd-9c52-e292c2ca47de",
   "metadata": {},
   "source": [
    "### Check distribution in redshift bins (integrate over the full mass range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da7552-6782-4340-971d-502a11a75405",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_z = np.histogram(ran_z, bins=50)\n",
    "\n",
    "print(logm_min, logm_max)\n",
    "predicted_T08_z_quad = [\n",
    "    norm\n",
    "    * N\n",
    "    * scipy.integrate.dblquad(\n",
    "        pdf, zzmin, zzmax, lambda x: logm_min, lambda x: logm_max, epsrel=1.0e-4\n",
    "    )[0]\n",
    "    for zzmin, zzmax in zip(hist_z[1], hist_z[1][1:])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1d07c-d75b-4d19-81ee-ec2283b8fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers_z = 0.5 * (hist_z[1][:-1] + hist_z[1][1:])\n",
    "\n",
    "plt.hist(\n",
    "    ran_z,\n",
    "    bins=hist_z[1],\n",
    "    color=\"orange\",\n",
    "    alpha=0.3,\n",
    "    label=\"drawn redshift - integrated over mass range\",\n",
    ")\n",
    "plt.plot(\n",
    "    bin_centers_z,\n",
    "    np.array(predicted_T08_z_quad),\n",
    "    \"-o\",\n",
    "    color=\"darkorange\",\n",
    "    label=\"Expected from theory\",\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Redshift\", size=14)\n",
    "plt.ylabel(\"Number\", size=14)\n",
    "plt.title(f\"{N:,} clusters drawn in log10M={logm_min, logm_max}, z={zmin, zmax}\")\n",
    "plt.legend(loc=4)\n",
    "plt.grid(which=\"major\", lw=0.5)\n",
    "plt.grid(which=\"minor\", lw=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60caeb",
   "metadata": {},
   "source": [
    "## Generating a cluster catalog and associated source catalogs\n",
    "- First, set the cluster masses and redshifts given the generated mock data. Only keep 30 clusters from N generated above to avoid memory issues.\n",
    "- Then, instantiate a CCL concentration object to compute the concentration for each cluster from a mass-concentration realtion (Duffy et al. 2008). The actual concentration for each cluster is drawn from a lognormal distribution around the mean value\n",
    "- Last, we randomly generate the cluster center coordinates over the full sky (from 0 to 360 deg for ra and from -90 to 90 deg to dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b181201-c866-4d07-9208-8828b28c15ff",
   "metadata": {},
   "source": [
    "### Cluster ensemble masses, redshifts and positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12092a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly keep 30 (M,z) pairs from the 1000s generated abve.\n",
    "Nstack = 30\n",
    "indices = np.random.choice(len(ran_logm), Nstack)\n",
    "cluster_m = 10 ** ran_logm[indices]\n",
    "cluster_z = ran_z[indices]\n",
    "\n",
    "# Concentration CCL object to compute the theoretical concentration\n",
    "conc_obj = ccl.halos.ConcentrationDuffy08(mass_def=hmd_200c)\n",
    "conc_list = []\n",
    "for number in range(0, len(cluster_m)):\n",
    "    a = 1.0 / (1.0 + (cluster_z[number]))\n",
    "    # mean value of the concentration for that cluster\n",
    "    lnc_mean = np.log(conc_obj(cosmo, M=(cluster_m[number]), a=a))\n",
    "    # random draw of actual concentration from normal distribution around lnc_mean, with a 0.14 scatter\n",
    "    lnc = np.random.normal(lnc_mean, 0.14)\n",
    "    conc_list.append(np.exp(lnc))\n",
    "\n",
    "conc_list = np.array(conc_list)\n",
    "\n",
    "# randomly draw cluster positions over the full sky\n",
    "ra = np.random.random(N) * 360  # from 0 to 360 deg\n",
    "sindec = np.random.random(N) * 2 - 1\n",
    "dec = np.arcsin(sindec) * 180 / np.pi  # from -90 to 90 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aeff73-c6aa-46b4-b0be-bef63dfd4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_m)\n",
    "Nstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ca501",
   "metadata": {},
   "source": [
    "### Background galaxy catalog generation\n",
    "\n",
    "For each cluster of the ensemble, we use `mock_data` to generate a background galaxy catalog and store the results in a `GalaxyCluster` object. Note that:\n",
    "- The cluster density profiles follow the NFW parametrisation\n",
    "- The source redshifts follow the Chang et al. distribution and have associated pdfs\n",
    "- The shapes include shape noise and shape measurement errors\n",
    "- Background galaxy catalogs are independent, even if the clusters are close (i.e., no common galaxy between two catalogs).\n",
    "- For each cluster we then compute\n",
    "    - the tangential and cross $\\Delta\\Sigma$ for each background galaxy\n",
    "    - the weights `w_ls` to be used to compute the corresponding radial profiles (see `demo_compute_deltasigma_weights.ipynb` notebook for details)\n",
    "\n",
    "The cluster objects are then stored in `gclist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207b191-43d5-45ab-88ad-36a3c235aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\n",
    "    \"ignore\"\n",
    ")  # just to prevent warning print out when looping over the cluster ensemble below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06709fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gclist = []\n",
    "# number of galaxies in each cluster field (alternatively, can use the galaxy density instead)\n",
    "n_gals = 10000\n",
    "# ngal_density = 10\n",
    "cosmo_clmm = Cosmology(H0=71.0, Omega_dm0=0.265 - 0.0448, Omega_b0=0.0448, Omega_k0=0.0)\n",
    "# cosmo_clmm.set_be_cosmo(cosmo)\n",
    "gal_args = dict(\n",
    "    cosmo=cosmo_clmm,\n",
    "    zsrc=\"chang13\",\n",
    "    delta_so=200,\n",
    "    massdef=\"critical\",\n",
    "    halo_profile_model=\"nfw\",\n",
    "    zsrc_max=3.0,\n",
    "    field_size=10.0,\n",
    "    shapenoise=0.04,\n",
    "    photoz_sigma_unscaled=0.02,\n",
    "    ngals=n_gals,\n",
    "    mean_e_err=0.1,\n",
    ")\n",
    "\n",
    "for i in range(Nstack):\n",
    "    # generate background galaxy catalog for cluster i\n",
    "    cl = clmm.GalaxyCluster(\n",
    "        f\"mock_cluster_{i:04}\",\n",
    "        ra[i],\n",
    "        dec[i],\n",
    "        cluster_z[i],\n",
    "        galcat=mock.generate_galaxy_catalog(\n",
    "            cluster_m[i],\n",
    "            cluster_z[i],\n",
    "            conc_list[i],\n",
    "            cluster_ra=ra[i],\n",
    "            cluster_dec=dec[i],\n",
    "            zsrc_min=cluster_z[i] + 0.1,\n",
    "            **gal_args,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # compute DeltaSigma for each background galaxy\n",
    "    cl.compute_tangential_and_cross_components(\n",
    "        shape_component1=\"e1\",\n",
    "        shape_component2=\"e2\",\n",
    "        tan_component=\"DS_t\",\n",
    "        cross_component=\"DS_x\",\n",
    "        cosmo=cosmo_clmm,\n",
    "        is_deltasigma=True,\n",
    "        use_pdz=True,\n",
    "    )\n",
    "\n",
    "    # compute the weights to be used to bluid the DeltaSigma radial profiles\n",
    "    cl.compute_galaxy_weights(\n",
    "        use_pdz=True,\n",
    "        use_shape_noise=True,\n",
    "        shape_component1=\"e1\",\n",
    "        shape_component2=\"e2\",\n",
    "        use_shape_error=True,\n",
    "        shape_component1_err=\"e_err\",\n",
    "        shape_component2_err=\"e_err\",\n",
    "        weight_name=\"w_ls\",\n",
    "        cosmo=cosmo_clmm,\n",
    "        is_deltasigma=True,\n",
    "        add=True,\n",
    "    )\n",
    "\n",
    "    # append the cluster in the list\n",
    "    gclist.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.galcat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e4bf7",
   "metadata": {},
   "source": [
    "## Creating ClusterEnsemble object and estimation of individual excess surface density profiles\n",
    "From the galaxy cluster object list `gclist`, we instantiate a cluster ensemble object `clusterensemble`. This instantiation step uses\n",
    " - the individual galaxy input $\\Delta\\Sigma_+$ and $\\Delta\\Sigma_{\\times}$ values (computed in the previous step, `DS_{t,x}`)\n",
    " - the corresponding individual input weights `w_ls` (computed in the previous step)\n",
    "\n",
    "to compute\n",
    "\n",
    "- the output tangential `DS_t` and cross signal `DS_x` binned profiles (where the binning is controlled by `bins`)\n",
    "- the associated profile weights `W_l` (that will be used to compute the stacked profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ensemble_id = 1\n",
    "bins = np.logspace(np.log10(0.3), np.log10(5), 10)\n",
    "clusterensemble = ClusterEnsemble(\n",
    "    ensemble_id,\n",
    "    gclist,\n",
    "    tan_component_in=\"DS_t\",\n",
    "    cross_component_in=\"DS_x\",\n",
    "    tan_component_out=\"DS_t\",\n",
    "    cross_component_out=\"DS_x\",\n",
    "    weights_in=\"w_ls\",\n",
    "    weights_out=\"W_l\",\n",
    "    bins=bins,\n",
    "    bin_units=\"Mpc\",\n",
    "    cosmo=cosmo_clmm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678f158",
   "metadata": {},
   "source": [
    "There is also the option to create an `ClusterEnsemble` object without the clusters list. Then, the user may compute the individual profile for each wanted cluster and compute the radial profile once all the indvidual profiles have been computed. This method may be reccomended if there a large number of clusters to avoid excess of memory allocation, since the user can generate each cluster separately, compute its individual profile and then delete the cluster object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155264ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_id2 = 2\n",
    "empty_cluster_ensemble = ClusterEnsemble(ensemble_id2)\n",
    "for cluster in gclist:\n",
    "    empty_cluster_ensemble.make_individual_radial_profile(\n",
    "        galaxycluster=cluster,\n",
    "        tan_component_in=\"DS_t\",\n",
    "        cross_component_in=\"DS_x\",\n",
    "        tan_component_out=\"DS_t\",\n",
    "        cross_component_out=\"DS_x\",\n",
    "        weights_in=\"w_ls\",\n",
    "        weights_out=\"W_l\",\n",
    "        bins=bins,\n",
    "        bin_units=\"Mpc\",\n",
    "        cosmo=cosmo_clmm,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368913e5",
   "metadata": {},
   "source": [
    "A third option is where all clusters already have the profile computed, and we just add those to the `ClusterEnsemble` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd473dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add profiles to gclist\n",
    "for cluster in gclist:\n",
    "    cluster.make_radial_profile(\n",
    "        tan_component_in=\"DS_t\",\n",
    "        cross_component_in=\"DS_x\",\n",
    "        tan_component_out=\"DS_t\",\n",
    "        cross_component_out=\"DS_x\",\n",
    "        weights_in=\"w_ls\",\n",
    "        weights_out=\"W_l\",\n",
    "        bins=bins,\n",
    "        bin_units=\"Mpc\",\n",
    "        cosmo=cosmo_clmm,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9986fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_id3 = 3\n",
    "empty_cluster_ensemble = ClusterEnsemble(ensemble_id3)\n",
    "for cluster in gclist:\n",
    "    empty_cluster_ensemble.add_individual_radial_profile(\n",
    "        galaxycluster=cluster,\n",
    "        profile_table=cluster.profile,\n",
    "        tan_component=\"DS_t\",\n",
    "        cross_component=\"DS_x\",\n",
    "        weights=\"W_l\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091de7c",
   "metadata": {},
   "source": [
    "### Stacked profile of the cluster ensemble\n",
    "The stacked radial profile of the ensemble is then obtained as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6162ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterensemble.make_stacked_radial_profile(tan_component=\"DS_t\", cross_component=\"DS_x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c1382",
   "metadata": {},
   "source": [
    "### Covariance (Bootstrap, sample, Jackknife) of the stack between radial bins\n",
    "Radial bins may be correlated and the `ClusterEnsemble` class provides three methods to compute the covariance matrix of the stacked signal, from the data:\n",
    "- The Sample covariance directly computes the covariance between radial bins of the `N` individual cluster profiles of the stack.\n",
    "- The Bootstrap approach is a resampling technique generating `n_bootstrap` ensembles of `N` randomly drawn clusters from the original ensemble, allowing for duplication. For each new ensemble, the stacked profile is computed and the covariance computed over the `n_bootstrap` stacks.\n",
    "- The Jackknife approach is another resampling technique, that divides the sky in a given number of regions $N_{\\rm region}$ and computes the covariance removing one region (i.e the clusters of the ensemble in that region) at a time. The stack is then computed using the remaining clusters and the covariance computed over the $N_{\\rm region}$ number of stacks. The division of the sky is done using the Healpix pixelisation scheme and is controlled by the `n_side` parameter, with $N_{\\rm region}=12 N_{\\rm side}^2$.\n",
    "\n",
    "NB: Approaches exist to compute the theoretical covariance of a stack but these are not (yet) available in `CLMM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterensemble.compute_sample_covariance(tan_component=\"DS_t\", cross_component=\"DS_x\")\n",
    "clusterensemble.compute_bootstrap_covariance(\n",
    "    tan_component=\"DS_t\", cross_component=\"DS_x\", n_bootstrap=300\n",
    ")\n",
    "clusterensemble.compute_jackknife_covariance(\n",
    "    n_side=16, tan_component=\"DS_t\", cross_component=\"DS_x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fab9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.rcParams[\"axes.linewidth\"] = 2\n",
    "plt.plot(\n",
    "    clusterensemble.data[\"radius\"][0],\n",
    "    clusterensemble.cov[\"tan_sc\"].diagonal() ** 0.5 / 1e13,\n",
    "    \"--\",\n",
    "    c=\"royalblue\",\n",
    "    label=\"Sample\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.plot(\n",
    "    clusterensemble.data[\"radius\"][0],\n",
    "    clusterensemble.cov[\"tan_jk\"].diagonal() ** 0.5 / 1e13,\n",
    "    \"-s\",\n",
    "    c=\"g\",\n",
    "    label=\"Bootstrap\",\n",
    "    linewidth=3,\n",
    "    markersize=10,\n",
    ")\n",
    "plt.plot(\n",
    "    clusterensemble.data[\"radius\"][0],\n",
    "    clusterensemble.cov[\"tan_sc\"].diagonal() ** 0.5 / 1e13,\n",
    "    c=\"r\",\n",
    "    label=\"Jackknife\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.xlabel(\"R [Mpc]\", fontsize=20)\n",
    "plt.ylabel(r\"$\\sigma_{\\Delta\\Sigma}\\ (\\times 10^{13} M_\\odot /Mpc^2)$\", fontsize=25)\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "plt.legend(frameon=False, fontsize=20)\n",
    "plt.loglog()\n",
    "plt.grid(which=\"major\", lw=0.5)\n",
    "plt.grid(which=\"minor\", lw=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "plt.rcParams[\"axes.linewidth\"] = 2\n",
    "fig.subplots_adjust(wspace=0.15, hspace=0)\n",
    "\n",
    "maximum = clusterensemble.cov[\"tan_sc\"].max()\n",
    "for ax, cov, label in zip(\n",
    "    axes,\n",
    "    [clusterensemble.cov[\"tan_sc\"], clusterensemble.cov[\"tan_jk\"], clusterensemble.cov[\"tan_sc\"]],\n",
    "    [\"Stack : Sample\", \"Stack : Bootstrap\", \"Stack : Jackknife\"],\n",
    "):\n",
    "    ax.set_title(label, fontsize=20)\n",
    "    ax.set_xlabel(\"radial bin index\", fontsize=18)\n",
    "    ax.set_ylabel(\"radial bin index\", fontsize=18)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    im = ax.imshow(cov, cmap=\"Reds\", vmin=0, vmax=maximum, origin=\"lower\")\n",
    "    plt.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da9c98",
   "metadata": {},
   "source": [
    "### Visualizing the stacked profiles and corresponding model\n",
    "In the figure below, we plot:\n",
    "- the individual $\\Delta\\Sigma$ profiles of the clusters (light blue)\n",
    "- the stacked signal (red symbols)\n",
    "- the prediction computed using a NFW profile and the mean values of the mass, concentration and redshift in the stack (dashed black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360010f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "moo = clmm.Modeling(massdef=\"critical\", delta_mdef=200, halo_profile_model=\"nfw\")\n",
    "moo.set_cosmo(cosmo_clmm)\n",
    "# Average values of mass and concentration of the ensemble to be used below\n",
    "# to overplot the model on the stacked profile\n",
    "moo.set_concentration(conc_list.mean())\n",
    "moo.set_mass(cluster_m.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a31b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_stack, gt_stack, gx_stack = (clusterensemble.stacked_data[c] for c in (\"radius\", \"DS_t\", \"DS_x\"))\n",
    "plt.rcParams[\"axes.linewidth\"] = 2\n",
    "fig, axs = plt.subplots(1, 2, figsize=(17, 6))\n",
    "\n",
    "err_gt = clusterensemble.cov[\"tan_sc\"].diagonal() ** 0.5 / 1e13\n",
    "err_gx = clusterensemble.cov[\"cross_sc\"].diagonal() ** 0.5 / 1e13\n",
    "\n",
    "axs[0].errorbar(\n",
    "    r_stack,\n",
    "    gt_stack / 1e13,\n",
    "    err_gt,\n",
    "    markersize=5,\n",
    "    c=\"r\",\n",
    "    fmt=\"o\",\n",
    "    capsize=10,\n",
    "    elinewidth=1,\n",
    "    zorder=1000,\n",
    "    alpha=1,\n",
    "    label=\"Stack\",\n",
    ")\n",
    "axs[1].errorbar(\n",
    "    r_stack,\n",
    "    gx_stack / 1e13,\n",
    "    err_gx,\n",
    "    markersize=5,\n",
    "    c=\"r\",\n",
    "    fmt=\"o\",\n",
    "    capsize=10,\n",
    "    elinewidth=1,\n",
    "    zorder=1000,\n",
    "    alpha=1,\n",
    "    label=\"Stack\",\n",
    ")\n",
    "# print(moo.eval_excess_surface_density(clusterensemble.data['radius'][0], cluster_z.mean())/1e13)\n",
    "axs[0].plot(\n",
    "    clusterensemble.data[\"radius\"][0],\n",
    "    moo.eval_excess_surface_density(clusterensemble.data[\"radius\"][0], cluster_z.mean()) / 1e13,\n",
    "    \"--k\",\n",
    "    linewidth=3,\n",
    "    label=\"Prediction from stack mean cluster\",\n",
    "    zorder=100,\n",
    ")\n",
    "axs[1].plot(\n",
    "    clusterensemble.data[\"radius\"][0],\n",
    "    0 * moo.eval_excess_surface_density(clusterensemble.data[\"radius\"][0], cluster_z.mean()) / 1e13,\n",
    "    \"--k\",\n",
    "    linewidth=3,\n",
    "    label=\"y=0\",\n",
    "    zorder=100,\n",
    ")\n",
    "\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[1].set_xscale(\"log\")\n",
    "\n",
    "\n",
    "for i in range(Nstack):\n",
    "    axs[0].plot(\n",
    "        clusterensemble.data[\"radius\"][i],\n",
    "        clusterensemble.data[\"DS_t\"][i] / 1e13,\n",
    "        color=\"cyan\",\n",
    "        label=\"Individual\",\n",
    "        alpha=1,\n",
    "        linewidth=1,\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        clusterensemble.data[\"radius\"][i],\n",
    "        clusterensemble.data[\"DS_x\"][i] / 1e13,\n",
    "        color=\"cyan\",\n",
    "        label=\"Individual\",\n",
    "        alpha=1,\n",
    "        linewidth=1,\n",
    "    )\n",
    "    if i == 0:\n",
    "        axs[0].legend(frameon=False, fontsize=15)\n",
    "        axs[1].legend(frameon=False, fontsize=15)\n",
    "# axs[0].plot(np.average(clusterensemble.data['radius'], axis = 0), np.average(clusterensemble.data['gt'], weights = None, axis = 0)/1e13)\n",
    "axs[0].set_xlabel(\"R [Mpc]\", fontsize=20)\n",
    "axs[1].set_xlabel(\"R [Mpc]\", fontsize=20)\n",
    "axs[0].tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "axs[1].tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "axs[0].set_ylabel(r\"$\\Delta\\Sigma_+$ $[\\times 10^{13} M_\\odot /Mpc^])$\", fontsize=20)\n",
    "axs[1].set_ylabel(r\"$\\Delta\\Sigma_\\times$  $[\\times 10^{13} M_\\odot /Mpc^2]$\", fontsize=20)\n",
    "axs[0].set_title(r\"Tangential\", fontsize=20)\n",
    "axs[1].set_title(r\"Cross\", fontsize=20)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(lw=0.5)\n",
    "    ax.grid(which=\"minor\", lw=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572d89e",
   "metadata": {},
   "source": [
    "## Saving/Loading ClusterEnsemble\n",
    "The `ClusterEnsemble` object also have an option for saving/loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterensemble.save(\"ce.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterensemble2 = ClusterEnsemble.load(\"ce.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
