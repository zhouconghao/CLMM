{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit halo mass to shear profile using Numcosmo statistical framework\n",
    "\n",
    "_the LSST-DESC CLMM team_\n",
    "\n",
    "This notebook demonstrates how to use `clmm` to estimate a WL halo mass from observations of a galaxy cluster. It uses several functionalities of the support `mock_data` module to produce datasets of increasing complexity. It relies on Numcosmo tools for the statistical analysis and shows how to fit for both cluster mass and concentration.\n",
    "\n",
    "1. The first part of notebook is equivalent to `Example2_Fit_Halo_Mass_to_Shear_Catalog.ipynb` and demonstrates the bias introduced on the reconstructed mass by a naive fit, when the redshift distribution of the background galaxies is not properly accounted for in the model.\n",
    "\n",
    "- Setting things up, with the proper imports.\n",
    "- Generating 3 datasets: an ideal dataset (dataset1) similar to that of Example1 (single source plane); an ideal dataset but with source galaxies following the Chang et al. (2013) redshift distribution (dataset2); a noisy dataset where photoz errors and shape noise are also included (dataset3). \n",
    "- Computing the binned reduced tangential shear profile, for the 3 datasets, using logarithmic binning.\n",
    "- Setting up the \"single source plane\" model to be fitted to the 3 datasets. Only dataset1 has a single source plane, so we expect to see a bias in the reconstructed mass when using this model on datasets 2 and 3. \n",
    "- Perform a simple fit using NumCosmo tools to compute the best-fit and the Fisher Matrix, and visualize the results.\n",
    "- Perform a MCMC analysis using NumCosmo tools and visualize the results.\n",
    "\n",
    "2. In a second part, an unbinned likelihood is build to properly account for the redshift distribution, in a manner differing from that presented in `Example3_Fit_Halo_Mass_to_Shear_Catalog.ipynb`. Both the best-fit+Fisher matrix and MCMC analyses are used.\n",
    "\n",
    "\n",
    "NB: to display the corner plot output of the MCMC analysis, you will need the `corner` package installed in your python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import some standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NumCosmo\n",
    "import os\n",
    "import sys\n",
    "import gi\n",
    "\n",
    "gi.require_version(\"NumCosmo\", \"1.0\")\n",
    "gi.require_version(\"NumCosmoMath\", \"1.0\")\n",
    "from gi.repository import GObject\n",
    "from gi.repository import NumCosmo as Nc\n",
    "from gi.repository import NumCosmoMath as Ncm\n",
    "\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import math\n",
    "\n",
    "# The corner package is needed to view the results of the MCMC analysis\n",
    "import corner\n",
    "\n",
    "\n",
    "os.environ[\"CLMM_MODELING_BACKEND\"] = \"nc\"\n",
    "\n",
    "__name__ = \"NcContext\"\n",
    "\n",
    "Ncm.cfg_init()\n",
    "Ncm.cfg_set_log_handler(lambda msg: sys.stdout.write(msg) and sys.stdout.flush())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clmm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from clmm.support.sampler import fitters\n",
    "\n",
    "clmm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import `clmm`'s core modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clmm.dataops as da\n",
    "import clmm.galaxycluster as gc\n",
    "import clmm.theory as theory\n",
    "from clmm import Cosmology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import a support modules for a specific data sets.\n",
    "`clmm` includes support modules that enable the user to generate mock data in a format compatible with `clmm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm.support import mock_data as mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making mock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create mock data, we need to define a true cosmology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_cosmo = Cosmology(H0=70.0, Omega_dm0=0.27 - 0.045, Omega_b0=0.045, Omega_k0=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set some parameters for a mock galaxy cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = mock_cosmo\n",
    "cluster_m = 1.0e15  # M200,m [Msun]\n",
    "cluster_z = 0.3  # Cluster's redshift\n",
    "concentration = 4\n",
    "ngals = 10000  # Number of galaxies\n",
    "Delta = 200\n",
    "cluster_ra = 0.0\n",
    "cluster_dec = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `mock_data` support module to generate 3 galaxy catalogs:\n",
    "- `ideal_data`: all background galaxies at the same redshift.\n",
    "- `ideal_data_z`: galaxies distributed according to the Chang et al. (2013) redshift distribution.\n",
    "- `noisy_data_z`: `ideal_data_z` + photoz errors + shape noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_data = mock.generate_galaxy_catalog(\n",
    "    cluster_m, cluster_z, concentration, cosmo, 0.8, ngals=ngals\n",
    ")\n",
    "ideal_data_z = mock.generate_galaxy_catalog(\n",
    "    cluster_m, cluster_z, concentration, cosmo, \"chang13\", ngals=ngals\n",
    ")\n",
    "noisy_data_z = mock.generate_galaxy_catalog(\n",
    "    cluster_m,\n",
    "    cluster_z,\n",
    "    concentration,\n",
    "    cosmo,\n",
    "    \"chang13\",\n",
    "    shapenoise=0.05,\n",
    "    photoz_sigma_unscaled=0.05,\n",
    "    ngals=ngals,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxy catalogs are converted to a `clmm.GalaxyCluster` object and may be saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = \"CL_ideal\"\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec, cluster_z, ideal_data)\n",
    "gc_object.save(\"ideal_GC.pkl\")\n",
    "\n",
    "cluster_id = \"CL_ideal_z\"\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec, cluster_z, ideal_data_z)\n",
    "gc_object.save(\"ideal_GC_z.pkl\")\n",
    "\n",
    "cluster_id = \"CL_noisy_z\"\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec, cluster_z, noisy_data_z)\n",
    "gc_object.save(\"noisy_GC_z.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any saved `clmm.GalaxyCluster` object may be read in for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = clmm.GalaxyCluster.load(\"ideal_GC.pkl\")  # all background galaxies at the same redshift\n",
    "cl2 = clmm.GalaxyCluster.load(\n",
    "    \"ideal_GC_z.pkl\"\n",
    ")  # background galaxies distributed according to Chang et al. (2013)\n",
    "cl3 = clmm.GalaxyCluster.load(\"noisy_GC_z.pkl\")  # same as cl2 but with photoz error and shape noise\n",
    "\n",
    "print(\"Cluster info = ID:\", cl2.unique_id, \"; ra:\", cl2.ra, \"; dec:\", cl2.dec, \"; z_l :\", cl2.z)\n",
    "print(\"The number of source galaxies is :\", len(cl2.galcat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(cl2.galcat[\"z\"], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing shear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clmm.dataops.compute_tangential_and_cross_components` calculates the tangential and cross shears for each source galaxy in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1, g_t1, g_x1 = cl1.compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "theta2, g_t2, g_x2 = cl2.compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "theta3, g_t3, g_x3 = cl3.compute_tangential_and_cross_components(geometry=\"flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radially binning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = da.make_bins(0.7, 4, 15, method=\"evenlog10width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clmm.dataops.make_radial_profile` evaluates the average shear of the galaxy catalog in bins of radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile1 = cl1.make_radial_profile(\"Mpc\", bins=bin_edges, cosmo=cosmo)\n",
    "profile2 = cl2.make_radial_profile(\"Mpc\", bins=bin_edges, cosmo=cosmo)\n",
    "profile3 = cl3.make_radial_profile(\"Mpc\", bins=bin_edges, cosmo=cosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `clmm.dataops.make_radial_profile` on a `clmm.GalaxyCluster` object, the object acquires the `clmm.GalaxyCluster.profile` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in cl1.profile.colnames:\n",
    "    cl1.profile[n].format = \"%6.3e\"\n",
    "cl1.profile.pprint(max_width=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the radially binned shear for the 3 configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "fsize = 14\n",
    "fig.gca().errorbar(\n",
    "    profile1[\"radius\"], profile1[\"gt\"], yerr=profile1[\"gt_err\"], marker=\"o\", label=\"z_src = 0.8\"\n",
    ")\n",
    "fig.gca().errorbar(\n",
    "    profile2[\"radius\"],\n",
    "    profile2[\"gt\"],\n",
    "    yerr=profile2[\"gt_err\"],\n",
    "    marker=\"o\",\n",
    "    label=\"z_src = Chang et al. (2013)\",\n",
    ")\n",
    "fig.gca().errorbar(\n",
    "    profile3[\"radius\"],\n",
    "    profile3[\"gt\"],\n",
    "    yerr=profile3[\"gt_err\"],\n",
    "    marker=\"o\",\n",
    "    label=\"z_src = Chang et al. (2013) + photoz err  + shape noise\",\n",
    ")\n",
    "\n",
    "plt.gca().set_title(r\"Binned shear of source galaxies\", fontsize=fsize)\n",
    "plt.gca().set_xlabel(r\"$r\\;[Mpc]$\", fontsize=fsize)\n",
    "plt.gca().set_ylabel(r\"$g_t$\", fontsize=fsize)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a halo mass - highlighting bias when not accounting for the source redshift distribution in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the best-fit mass using a simple implementation of the likelihood using a NcmDataGaussDiag object.\n",
    "\n",
    "Here, to build the model we make the WRONG assumption that the average shear in bin $i$ equals the shear at the average redshift in the bin; i.e. we assume that $\\langle g_t\\rangle_i = g_t(\\langle z\\rangle_i)$. This should not impact `cluster 1` as all sources are located at the same redshift. However, this yields a bias in the constructed mass for `cluster 2` and `cluster 3`, where the sources followed the Chang et al. (2013) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the reconstructed mass is biased whenever the sources are not located at a single redshift as this was not accounted for in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the halo model\n",
    "\n",
    "Here we model using the OO inteface, we also use NumCosmo statistical framework to perform the analysis. Below we create an object based on NumCosmo NcmDataGaussDiag (Gaussian likelihood with a diagonal covariance matrix) object. To connect with the C interface the object must implement the methods: `do_get_length`, `do_get_dof`, `do_begin`, `do_prepare` and `do_mean_func`. The last method is responsible to compute the theoretical predictions. In the param_set_ftype calls below one can change between FREE/FIXED to include/exclude the parameter from the analysis. \n",
    "\n",
    "Remember that here we are building the wrong model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussGammaTErr(Ncm.DataGaussDiag):\n",
    "    z_cluster = GObject.Property(type=float, flags=GObject.PARAM_READWRITE)\n",
    "    z_src = GObject.Property(type=Ncm.Vector, flags=GObject.PARAM_READWRITE)\n",
    "    r_source = GObject.Property(type=Ncm.Vector, flags=GObject.PARAM_READWRITE)\n",
    "    z_err = GObject.Property(type=Ncm.Vector, flags=GObject.PARAM_READWRITE)\n",
    "\n",
    "    def __init__(self):\n",
    "        Ncm.DataGaussDiag.__init__(self, n_points=0)\n",
    "        self.moo = clmm.Modeling()\n",
    "\n",
    "    def init_from_data(self, z_cluster, r_source, z_src, gt_profile, gt_err, z_err=None, moo=None):\n",
    "        if moo:\n",
    "            self.moo = moo\n",
    "\n",
    "        assert len(gt_profile) == len(z_src)\n",
    "        assert len(gt_profile) == len(r_source)\n",
    "        assert len(gt_profile) == len(gt_err)\n",
    "\n",
    "        self.set_size(len(gt_profile))\n",
    "\n",
    "        self.props.z_cluster = z_cluster\n",
    "        self.props.z_src = Ncm.Vector.new_array(z_src)\n",
    "        self.props.r_source = Ncm.Vector.new_array(r_source)\n",
    "        if z_err:\n",
    "            self.props.r_source = Ncm.Vector.new_array(z_err)\n",
    "\n",
    "        self.y.set_array(gt_profile)\n",
    "\n",
    "        self.sigma.set_array(\n",
    "            gt_err\n",
    "        )  # Diagonal covariance matrix: standard deviation values in gt_err.\n",
    "\n",
    "        self.set_init(True)\n",
    "\n",
    "    # Once the NcmDataGaussDiag is initialized, its parent class variable np is set with the n_points value.\n",
    "    def do_get_length(self):\n",
    "        return self.np\n",
    "\n",
    "    def do_get_dof(self):\n",
    "        return self.np\n",
    "\n",
    "    def do_begin(self):\n",
    "        pass\n",
    "\n",
    "    def do_prepare(self, mset):\n",
    "        self.moo.set_mset(mset)\n",
    "\n",
    "    def do_mean_func(self, mset, vp):\n",
    "        vp.set_array(\n",
    "            self.moo.eval_reduced_tangential_shear(\n",
    "                self.props.r_source.dup_array(),\n",
    "                self.props.z_cluster,\n",
    "                self.props.z_src.dup_array(),\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "\n",
    "GObject.type_register(GaussGammaTErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model set (NcmMset), data set (NcmDataset) and NcmLikelihood objects to carry out a statistical analysis. \n",
    "\n",
    "The method `param_set_ftype` defines the parameters that can be fitted: `mid` - to which model set the parameter belongs to, `pid` - parameters' id, NcmParamType (FREE or FIXED) to say if the parameter will be fitted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moo1 = clmm.Modeling(massdef=\"mean\", delta_mdef=200, halo_profile_model=\"nfw\")\n",
    "moo1.set_cosmo(cosmo)\n",
    "\n",
    "moo2 = clmm.Modeling(massdef=\"mean\", delta_mdef=200, halo_profile_model=\"nfw\")\n",
    "moo2.set_cosmo(cosmo)\n",
    "\n",
    "moo3 = clmm.Modeling(massdef=\"mean\", delta_mdef=200, halo_profile_model=\"nfw\")\n",
    "moo3.set_cosmo(cosmo)\n",
    "\n",
    "ggt1 = GaussGammaTErr()\n",
    "ggt2 = GaussGammaTErr()\n",
    "ggt3 = GaussGammaTErr()\n",
    "\n",
    "ggt1.init_from_data(\n",
    "    z_cluster=cluster_z,\n",
    "    r_source=profile1[\"radius\"],\n",
    "    z_src=profile1[\"z\"],\n",
    "    gt_profile=profile1[\"gt\"],\n",
    "    gt_err=profile1[\"gt_err\"],\n",
    "    moo=moo1,\n",
    ")\n",
    "ggt2.init_from_data(\n",
    "    z_cluster=cluster_z,\n",
    "    r_source=profile2[\"radius\"],\n",
    "    z_src=profile2[\"z\"],\n",
    "    gt_profile=profile2[\"gt\"],\n",
    "    gt_err=profile2[\"gt_err\"],\n",
    "    moo=moo2,\n",
    ")\n",
    "ggt3.init_from_data(\n",
    "    z_cluster=cluster_z,\n",
    "    r_source=profile3[\"radius\"],\n",
    "    z_src=profile3[\"z\"],\n",
    "    gt_profile=profile3[\"gt\"],\n",
    "    gt_err=profile3[\"gt_err\"],\n",
    "    moo=moo3,\n",
    ")\n",
    "\n",
    "mset1 = ggt1.moo.get_mset()\n",
    "mset2 = ggt2.moo.get_mset()\n",
    "mset3 = ggt3.moo.get_mset()\n",
    "\n",
    "# Parameters: cluster mass (log base 10) and concentration\n",
    "MDelta_pi = mset1.param_get_by_full_name(\"NcHaloDensityProfile:log10MDelta\")\n",
    "cDelta_pi = mset1.param_get_by_full_name(\"NcHaloDensityProfile:cDelta\")\n",
    "\n",
    "mset1.param_set_ftype(MDelta_pi.mid, MDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset1.param_set_ftype(cDelta_pi.mid, cDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset1.prepare_fparam_map()\n",
    "\n",
    "mset2.param_set_ftype(MDelta_pi.mid, MDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset2.param_set_ftype(cDelta_pi.mid, cDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset2.prepare_fparam_map()\n",
    "\n",
    "mset3.param_set_ftype(MDelta_pi.mid, MDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset3.param_set_ftype(cDelta_pi.mid, cDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset3.prepare_fparam_map()\n",
    "\n",
    "dset1 = Ncm.Dataset.new()\n",
    "dset1.append_data(ggt1)\n",
    "lh1 = Ncm.Likelihood.new(dset1)\n",
    "\n",
    "dset2 = Ncm.Dataset.new()\n",
    "dset2.append_data(ggt2)\n",
    "lh2 = Ncm.Likelihood.new(dset2)\n",
    "\n",
    "dset3 = Ncm.Dataset.new()\n",
    "dset3.append_data(ggt3)\n",
    "lh3 = Ncm.Likelihood.new(dset3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting parameters: Fisher Matrix\n",
    "\n",
    "The NcmFit object receives the NcmLikelihood and NcmMset objects. The user also indicates the fitting algorithm and the numerical differentiation method.  \n",
    "Functions `run` and `fisher` computes the [best-fit](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) and the [fisher matrix](https://en.wikipedia.org/wiki/Fisher_information#Multivariate_normal_distribution), respectively. `log_info` prints the complete information about the data used, models and its parameters, and `log_covar` prints the best-fit along with the error-bar and the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = Ncm.Fit.new(Ncm.FitType.NLOPT, \"ln-neldermead\", lh1, mset1, Ncm.FitGradType.NUMDIFF_FORWARD)\n",
    "fit2 = Ncm.Fit.new(Ncm.FitType.NLOPT, \"ln-neldermead\", lh2, mset2, Ncm.FitGradType.NUMDIFF_FORWARD)\n",
    "fit3 = Ncm.Fit.new(Ncm.FitType.NLOPT, \"ln-neldermead\", lh3, mset3, Ncm.FitGradType.NUMDIFF_FORWARD)\n",
    "\n",
    "fit1.run(Ncm.FitRunMsgs.SIMPLE)\n",
    "fit1.fisher()\n",
    "fit1.log_info()\n",
    "fit1.log_covar()\n",
    "\n",
    "fit2.run(Ncm.FitRunMsgs.SIMPLE)\n",
    "fit2.fisher()\n",
    "fit2.log_info()\n",
    "fit2.log_covar()\n",
    "\n",
    "fit3.run(Ncm.FitRunMsgs.SIMPLE)\n",
    "fit3.fisher()\n",
    "fit3.log_info()\n",
    "fit3.log_covar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the results\n",
    "\n",
    "For visualization purpose, we calculate the reduced tangential shear predicted by the model when using the average redshift of the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = np.logspace(-0.5, np.log10(5), 100)\n",
    "\n",
    "gt_model1 = moo1.eval_reduced_tangential_shear(rr, cluster_z, np.mean(cl1.galcat[\"z\"]))\n",
    "gt_model2 = moo2.eval_reduced_tangential_shear(rr, cluster_z, np.mean(cl2.galcat[\"z\"]))\n",
    "gt_model3 = moo3.eval_reduced_tangential_shear(rr, cluster_z, np.mean(cl3.galcat[\"z\"]))\n",
    "\n",
    "m_est1 = 10 ** mset1.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "m_est2 = 10 ** mset2.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "m_est3 = 10 ** mset3.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "\n",
    "# Standard deviation\n",
    "m_est_err1 = fit1.covar_sd(MDelta_pi.mid, MDelta_pi.pid) * m_est1 * math.log(10.0)\n",
    "m_est_err2 = fit2.covar_sd(MDelta_pi.mid, MDelta_pi.pid) * m_est2 * math.log(10.0)\n",
    "m_est_err3 = fit3.covar_sd(MDelta_pi.mid, MDelta_pi.pid) * m_est3 * math.log(10.0)\n",
    "\n",
    "print(\n",
    "    \"% 22.15e +/- %.0e % 22.15e +/- %.0e % 22.15e +/- %.0e\"\n",
    "    % (m_est1, m_est_err1, m_est2, m_est_err2, m_est3, m_est_err3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize that prediction of reduced tangential shears along with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "axes[0].errorbar(\n",
    "    profile1[\"radius\"],\n",
    "    profile1[\"gt\"],\n",
    "    profile1[\"gt_err\"],\n",
    "    color=\"red\",\n",
    "    label=\"ideal_data, M_input = %.3e Msun\" % cluster_m,\n",
    "    fmt=\".\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    rr,\n",
    "    gt_model1,\n",
    "    color=\"red\",\n",
    "    label=\"best fit model 1, M_fit = %.2e +/- %.2e\" % (m_est1, m_est_err1),\n",
    ")\n",
    "\n",
    "\n",
    "axes[0].errorbar(\n",
    "    profile2[\"radius\"],\n",
    "    profile2[\"gt\"],\n",
    "    profile2[\"gt_err\"],\n",
    "    color=\"green\",\n",
    "    label=\"ideal_data_z, M_input = %.3e Msun\" % cluster_m,\n",
    "    fmt=\".\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    rr,\n",
    "    gt_model2,\n",
    "    color=\"green\",\n",
    "    label=\"best fit model 2, M_fit = %.2e +/- %.2e\" % (m_est2, m_est_err2),\n",
    ")\n",
    "axes[0].set_title(\"Ideal data w/wo src redshift distribution\", fontsize=fsize)\n",
    "axes[0].semilogx()\n",
    "axes[0].semilogy()\n",
    "axes[0].legend(fontsize=fsize)\n",
    "axes[0].set_xlabel(\"R [Mpc]\", fontsize=fsize)\n",
    "axes[0].set_ylabel(\"reduced tangential shear\", fontsize=fsize)\n",
    "\n",
    "axes[1].errorbar(\n",
    "    profile3[\"radius\"],\n",
    "    profile3[\"gt\"],\n",
    "    profile3[\"gt_err\"],\n",
    "    color=\"red\",\n",
    "    label=\"noisy_data_z, M_input = %.3e Msun\" % cluster_m,\n",
    "    fmt=\".\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    rr,\n",
    "    gt_model3,\n",
    "    color=\"red\",\n",
    "    label=\"best fit model 3, M_fit = %.2e +/- %.2e\" % (m_est3, m_est_err3),\n",
    ")\n",
    "axes[1].set_title(\"Noisy data with src redshift distribution\", fontsize=fsize)\n",
    "axes[1].semilogx()\n",
    "axes[1].semilogy()\n",
    "axes[1].legend(fontsize=fsize)\n",
    "axes[1].set_xlabel(\"R [Mpc]\", fontsize=fsize)\n",
    "axes[1].set_ylabel(\"reduced tangential shear\", fontsize=fsize)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To investigate further the results we make a MCMC analysis below.\n",
    "\n",
    "We begin by specifying if the run is single- or multi-thread: `func_eval_set_max_threads` sets the maximum number of threads, and `func_eval_log_pool_stats` prints the information about the thread pool.\n",
    "\n",
    "Then, we initialize the transition kernel object (NcmMSetTransKern) which defines the distribution of the initial points of the parameter space to be used by the ensemble sampler. In this example we use the Gaussian transition kernel (NcmMSetTransKernGauss), with priors provided by the NcmMset (`set_prior_from_mset`). `set_cov_from_rescale` sets the covariance matrix with zero correlation and the diagonal terms defined by the scale of each parameter times the argument of `set_cov_from_rescale`. \n",
    "\n",
    "Here we use the Ensemble Sampler MCMC (ESMCMC) method. `nwalkers` and `walker` define the number of walkers and the algorithm used to move the points in the ensemble. \n",
    "Running: `start_run`, `run_lre` and `end_run`. `run_lre` runs the ESMCMC until the relative error of the mean of each parameter is smaller than $10^{-3}$. Its first argument (integer) indicates how many ensembles are computed before applying any convergence test.\n",
    "\n",
    "In the end we save the catalog to mcat_wrong to compare with a correct analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncm.func_eval_set_max_threads(0)\n",
    "Ncm.func_eval_log_pool_stats()\n",
    "\n",
    "init_sampler = Ncm.MSetTransKernGauss.new(0)\n",
    "init_sampler.set_mset(mset3)\n",
    "init_sampler.set_prior_from_mset()\n",
    "init_sampler.set_cov_from_rescale(1.0e-1)\n",
    "\n",
    "nwalkers = 100  # Number of walkers\n",
    "walker = Ncm.FitESMCMCWalkerAPS.new(nwalkers, mset3.fparams_len())\n",
    "\n",
    "# Ensemble Sampler MCMC\n",
    "esmcmc = Ncm.FitESMCMC.new(fit3, nwalkers, init_sampler, walker, Ncm.FitRunMsgs.SIMPLE)\n",
    "esmcmc.set_data_file(\"example2_fit3_wrong_esmcmc_out_aps.fits\")\n",
    "esmcmc.set_auto_trim(True)  # Detect and discard the burn-in points.\n",
    "esmcmc.set_auto_trim_div(100)\n",
    "esmcmc.set_max_runs_time(2.0 * 60.0)  # Maximum time between tests.\n",
    "\n",
    "esmcmc.start_run()\n",
    "esmcmc.run_lre(20, 1.0e-3)\n",
    "esmcmc.end_run()\n",
    "\n",
    "mcat_wrong = esmcmc.peek_catalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct non- projected model\n",
    "\n",
    "Here, instead of building an object directly on top of NcmDataGauss*, we use NumCosmo's framework to build non-binned likelihood for weak-lensing cluster analysis.\n",
    "\n",
    "For that we need two objects: a NcGalaxyWLReducedShearGauss that model a Gaussian distributed reduced shear likelihood, here the observables matrix is simply $(r, \\gamma_t, \\sigma_{\\gamma_t})$ for each galaxy. If the data has spectroscopic redshifts then we use NcGalaxyRedshiftSpec with an array of real redshifts. When photometric errors are included we use the NcGalaxyRedshiftGauss object that receives $(z, \\sigma_z)$ for each galaxy. \n",
    "\n",
    "Once we have the data objects ready we can proceed as in the previous examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nc_data_cluster_wl(\n",
    "    theta, g_t, z_src, z_cluster, cosmo, dist, sigma_z=None, sigma_g=None\n",
    "):\n",
    "    r = clmm.convert_units(theta, \"radians\", \"Mpc\", redshift=z_cluster, cosmo=cosmo)\n",
    "    ga = Ncm.ObjArray.new()\n",
    "\n",
    "    sigma_g = 1.0e-4 if not sigma_g else sigma_g\n",
    "    m_obs = np.column_stack((r, g_t, np.repeat(sigma_g, len(r))))\n",
    "\n",
    "    grsg = Nc.GalaxyWLReducedShearGauss(pos=Nc.GalaxyWLReducedShearGaussPos.R)\n",
    "    grsg.set_obs(Ncm.Matrix.new_array(m_obs.flatten(), 3))\n",
    "\n",
    "    if sigma_z:\n",
    "        gzgs = Nc.GalaxyRedshiftGauss()\n",
    "        z_obs = np.column_stack((z_src, (1.0 + z_src) * sigma_z))\n",
    "        gzgs.set_obs(Ncm.Matrix.new_array(z_obs.flatten(), 2))\n",
    "    else:\n",
    "        gzgs = Nc.GalaxyRedshiftSpec()\n",
    "        gzgs.set_z(Ncm.Vector.new_array(z_src))\n",
    "\n",
    "    gwl = Nc.GalaxyWL(wl_dist=grsg, gz_dist=gzgs)\n",
    "    ga.add(gwl)\n",
    "\n",
    "    nc_dcwl = Nc.DataClusterWL(galaxy_array=ga, z_cluster=z_cluster)\n",
    "    nc_dcwl.set_init(True)\n",
    "\n",
    "    return nc_dcwl\n",
    "\n",
    "\n",
    "def create_fit_obj(data_array, mset):\n",
    "    dset = Ncm.Dataset.new()\n",
    "    for data in data_array:\n",
    "        dset.append_data(data)\n",
    "    lh = Ncm.Likelihood.new(dset)\n",
    "    fit = Ncm.Fit.new(Ncm.FitType.NLOPT, \"ln-neldermead\", lh, mset, Ncm.FitGradType.NUMDIFF_FORWARD)\n",
    "    # fit.set_params_reltol (1.0e-8)\n",
    "    # fit.set_m2lnL_reltol (1.0e-11)\n",
    "\n",
    "    return fit\n",
    "\n",
    "\n",
    "ggt1 = create_nc_data_cluster_wl(\n",
    "    theta1, g_t1, cl1.galcat[\"z\"], cluster_z, cosmo, cosmo.dist, sigma_z=None, sigma_g=None\n",
    ")\n",
    "ggt2 = create_nc_data_cluster_wl(\n",
    "    theta2, g_t2, cl2.galcat[\"z\"], cluster_z, cosmo, cosmo.dist, sigma_z=None, sigma_g=None\n",
    ")\n",
    "ggt3 = create_nc_data_cluster_wl(\n",
    "    theta3, g_t3, cl3.galcat[\"z\"], cluster_z, cosmo, cosmo.dist, sigma_z=0.05, sigma_g=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the likelihood is not Gaussian, here we compute the [Observed Fisher Matrix](https://en.wikipedia.org/wiki/Observed_information) (`obs_fisher`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = create_fit_obj([ggt1], mset1)\n",
    "fit2 = create_fit_obj([ggt2], mset2)\n",
    "fit3 = create_fit_obj([ggt3], mset3)\n",
    "\n",
    "fit1.run(Ncm.FitRunMsgs.SIMPLE)\n",
    "fit1.obs_fisher()\n",
    "fit1.log_covar()\n",
    "\n",
    "fit2.run(Ncm.FitRunMsgs.SIMPLE)\n",
    "fit2.obs_fisher()\n",
    "fit2.log_covar()\n",
    "\n",
    "fit3.run(Ncm.FitRunMsgs.SIMPLE)\n",
    "fit3.obs_fisher()\n",
    "fit3.log_covar()\n",
    "\n",
    "mest1 = 10 ** mset1.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "mest2 = 10 ** mset2.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "mest3 = 10 ** mset3.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "\n",
    "print(\"% 22.15e % 22.15e % 22.15e\" % (mest1, mest2, mest3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results\n",
    "\n",
    "Note below that we no longer have biased results, all results are well within the error bars. Note also that the error bars are substantially smaller than in the binned case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = np.logspace(-0.5, np.log10(5), 100)\n",
    "\n",
    "gt_model1 = moo1.eval_reduced_tangential_shear(rr, cluster_z, np.mean(cl1.galcat[\"z\"]))\n",
    "gt_model2 = moo2.eval_reduced_tangential_shear(rr, cluster_z, np.mean(cl2.galcat[\"z\"]))\n",
    "gt_model3 = moo3.eval_reduced_tangential_shear(rr, cluster_z, np.mean(cl3.galcat[\"z\"]))\n",
    "\n",
    "m_est1 = 10 ** mset1.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "m_est2 = 10 ** mset2.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "m_est3 = 10 ** mset3.param_get(MDelta_pi.mid, MDelta_pi.pid)\n",
    "\n",
    "m_est_err1 = fit1.covar_sd(MDelta_pi.mid, MDelta_pi.pid) * m_est1 * math.log(10.0)\n",
    "m_est_err2 = fit2.covar_sd(MDelta_pi.mid, MDelta_pi.pid) * m_est2 * math.log(10.0)\n",
    "m_est_err3 = fit3.covar_sd(MDelta_pi.mid, MDelta_pi.pid) * m_est3 * math.log(10.0)\n",
    "\n",
    "print(\n",
    "    \"% 22.15e +/- %.0e % 22.15e +/- %.0e % 22.15e +/- %.0e\"\n",
    "    % (m_est1, m_est_err1, m_est2, m_est_err2, m_est3, m_est_err3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To investigate further the results we make a MCMC analysis below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncm.func_eval_set_max_threads(0)\n",
    "Ncm.func_eval_log_pool_stats()\n",
    "\n",
    "init_sampler = Ncm.MSetTransKernGauss.new(0)\n",
    "init_sampler.set_mset(mset3)\n",
    "init_sampler.set_prior_from_mset()\n",
    "init_sampler.set_cov_from_rescale(1.0e-1)\n",
    "\n",
    "nwalkers = 100\n",
    "stretch = Ncm.FitESMCMCWalkerAPS.new(nwalkers, mset3.fparams_len())\n",
    "\n",
    "esmcmc = Ncm.FitESMCMC.new(fit3, nwalkers, init_sampler, stretch, Ncm.FitRunMsgs.SIMPLE)\n",
    "esmcmc.set_data_file(\"example2_fit3_esmcmc_out_aps.fits\")\n",
    "esmcmc.set_auto_trim(True)\n",
    "esmcmc.set_auto_trim_div(100)\n",
    "esmcmc.set_max_runs_time(2.0 * 60.0)\n",
    "\n",
    "esmcmc.start_run()\n",
    "esmcmc.run_lre(20, 1.0e-3)\n",
    "esmcmc.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we plot both MCMC results\n",
    "\n",
    "The wrong analysis has a strong bias in $\\log_{10}(M_\\Delta)$ (the peak of the wrong model is more than $3\\sigma$ away from the correct model best-fit) and much larger variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1d1 = chi2.cdf(1.0, df=1)\n",
    "s1d2 = chi2.cdf(4.0, df=1)\n",
    "s2d1 = chi2.cdf(1.0, df=2)\n",
    "s2d2 = chi2.cdf(4.0, df=2)\n",
    "s2d3 = chi2.cdf(9.0, df=2)\n",
    "qts = [0.5 - s1d2 / 2.0, 0.5 - s1d1 / 2.0, 0.5, 0.5 + s1d1 / 2.0, 0.5 + s1d2 / 2.0]\n",
    "\n",
    "mcat = esmcmc.peek_catalog()\n",
    "rows = np.array([mcat.peek_row(i).dup_array() for i in range(nwalkers * 10, mcat.len())])\n",
    "params = [\"$\" + mcat.col_symb(i) + \"$\" for i in range(mcat.ncols())]\n",
    "figure = corner.corner(\n",
    "    rows[:, 1:],\n",
    "    labels=params[1:],\n",
    "    reverse=False,\n",
    "    quantiles=qts,\n",
    "    levels=(s2d1, s2d2, s2d3),\n",
    "    bins=40,\n",
    "    smooth=0.8,\n",
    "    smooth1d=0.8,\n",
    "    color=(0.5, 0.2, 0.5, 1.0),\n",
    ")\n",
    "\n",
    "rows = np.array(\n",
    "    [mcat_wrong.peek_row(i).dup_array() for i in range(nwalkers * 10, mcat_wrong.len())]\n",
    ")\n",
    "params = [\"$\" + mcat_wrong.col_symb(i) + \"$\" for i in range(mcat_wrong.ncols())]\n",
    "figure = corner.corner(\n",
    "    rows[:, 1:],\n",
    "    labels=params[1:],\n",
    "    range=[(2.9, 9.5), (14.8, 15.12)],\n",
    "    reverse=False,\n",
    "    levels=(s2d1, s2d2, s2d3),\n",
    "    color=(0.1, 0.2, 0.5, 0.5),\n",
    "    bins=40,\n",
    "    smooth=0.8,\n",
    "    smooth1d=0.8,\n",
    "    fig=figure,\n",
    ")\n",
    "\n",
    "\n",
    "figure.set_size_inches(12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = Ncm.Serialize.new(0)\n",
    "data = fit3.lh.dset.get_data(0)\n",
    "ser.to_file(data, \"example2_fit3_data.obj\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
